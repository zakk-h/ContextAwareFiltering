{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70b67e-dae6-4ab5-a01a-cf93018319a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary bag of words representation of input data\n",
    "\n",
    "SPINE_INTERACTION = False # decide whether to start off with doubling the parameters for the model by doing bag-of-words embeddings and their interaction with the spine earlier flag\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# expected format\n",
    "# File Path column (to differentiate between reports)\n",
    "# Sentence Number (within report)\n",
    "# Sentence (the actual contents of the sentence)\n",
    "# Brain Related: 1 for yes, 0 for no\n",
    "# spine_earlier (flag for whether certain keywords relating to the spine have appeared earlier in the document) - can also just pass a constant to guarantee not used in models\n",
    "\n",
    "main_file = \"ml/brain_sentences.csv\"\n",
    "supplementary_file = \"ml/brain_sentences_supplementary.csv\"\n",
    "\n",
    "df_main = pd.read_csv(main_file) if os.path.exists(main_file) else pd.DataFrame()\n",
    "df_supplementary = pd.read_csv(supplementary_file) if os.path.exists(supplementary_file) else pd.DataFrame()\n",
    "\n",
    "if not df_main.empty and not df_supplementary.empty:\n",
    "    df = pd.concat([df_main, df_supplementary], ignore_index=True)\n",
    "elif not df_main.empty:\n",
    "    df = df_main\n",
    "elif not df_supplementary.empty:\n",
    "    df = df_supplementary\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "df.to_csv(\"ml/brain_sentences_combined_full.csv\", index=False)\n",
    "print(f\"Total rows after adding supplementary data: {len(df)}\")\n",
    "\n",
    "# boilerplate, won't input to model.\n",
    "phrases_to_remove = [\n",
    "    r'findings were discussed\\s+(?:with|at|by)',\n",
    "    r'preliminary report',\n",
    "    r'dose reduction was obtained',\n",
    "    r'no substantial differences',\n",
    "    r'provided by .* to .* at',\n",
    "    r'The preliminary report'\n",
    "    r'electronically reviewed',\n",
    "    r'electronically signed',\n",
    "    r'discussed with .*? by .*? at',\n",
    "    r'if AEC could not be utilized then',\n",
    "    r'^contrast dose:?(?:\\s+\\S+){0,2}\\.?$',  # ensures \"contrast dose\" is at the start, allows up to 2 more words, and an optional period and colon\n",
    "    r'^COMPARISON:(?:\\s+\\S+){0,2}\\.?$'  # similarly, COMPARISON, require colon, etc\n",
    "]\n",
    "\n",
    "# compile the patterns into one regex pattern (using OR)\n",
    "pattern = re.compile(\"|\".join(phrases_to_remove), re.IGNORECASE)\n",
    "\n",
    "# boolean mask of True iff \"Sentence\" contains any of the patterns\n",
    "mask = df[\"Sentence\"].str.contains(pattern, na=False)\n",
    "\n",
    "# drop rows where the mask is True\n",
    "df = df[~mask]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.to_csv(\"ml/brain_sentences_combined.csv\", index=False)\n",
    "print(f\"Total rows after adding supplementary data and removing generic sentences: {len(df)}\")\n",
    "\n",
    "# preprocess sentences: lowercase and remove punctuation\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df[\"Cleaned Sentence\"] = df[\"Sentence\"].apply(clean_text)\n",
    "\n",
    "# countvectorizer for a binary (frequency-invariant) bag-of-words representation\n",
    "vectorizer = CountVectorizer(binary=True, stop_words=\"english\")\n",
    "X_bow = vectorizer.fit_transform(df[\"Cleaned Sentence\"])  # transform sentences into a bow matrix\n",
    "\n",
    "# add features not explicitly contained in a sentence\n",
    "sentence_feature = (df[\"Sentence Number\"] <= 3).astype(int).values.reshape(-1, 1)\n",
    "spine_feature = df[\"spine_earlier\"].astype(int).values.reshape(-1, 1)\n",
    "\n",
    "if hasattr(vectorizer, \"get_feature_names_out\"):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "else:\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=feature_names)\n",
    "\n",
    "# compute word frequency across sentences\n",
    "word_counts = df_bow.sum(axis=0)\n",
    "\n",
    "# filter out columns (words) that appear fewer than 5 times to prevent overfitting - arbitrary cutoff and regularization should make it so it doesn't matter\n",
    "words_to_keep = word_counts[word_counts >= 5].index.tolist()\n",
    "\n",
    "df_final = pd.concat(\n",
    "    [df_bow[words_to_keep], \n",
    "     pd.DataFrame(sentence_feature, columns=[\"Sent_Num_LE3\"]),\n",
    "     pd.DataFrame(spine_feature, columns=[\"spine_earlier\"])], axis=1\n",
    ")\n",
    "\n",
    "# insert 'Brain Related' as the first column (the response variable)\n",
    "df_final.insert(0, \"Brain Related\", df[\"Brain Related\"].astype(int))\n",
    "\n",
    "if SPINE_INTERACTION:\n",
    "    # all cols except the response and raw spine flag\n",
    "    feature_cols = df_final.columns.drop([\"Brain Related\", \"spine_earlier\"]) # words and sentencenum<=3\n",
    "    # build interactions: each feature x spine_earlier\n",
    "    interactions = df_final[feature_cols].multiply(df_final[\"spine_earlier\"], axis=0)\n",
    "    # rename them\n",
    "    interactions.columns = [f\"{col}_x_spine\" for col in feature_cols]\n",
    "    # append \n",
    "    df_final = pd.concat([df_final, interactions], axis=1)\n",
    "\n",
    "X_sparse = csr_matrix(df_final)\n",
    "\n",
    "df_final.to_csv(\"ml/brain_sentences_sparse.csv\", index=False)\n",
    "print(\"Sparse BoW representation saved as ml/brain_sentences_sparse.csv.\")\n",
    "print(f\"Columns: {df_final.columns}\")\n",
    "\n",
    "# save for later use\n",
    "filtered_vectorizer = CountVectorizer(binary=True, stop_words=\"english\", vocabulary=words_to_keep)\n",
    "joblib.dump(filtered_vectorizer, \"ml/local_vectorizer.pkl\")\n",
    "print(\"Vectorizer saved as ml/local_vectorizer.pkl.\")\n",
    "\n",
    "prop_brain_related = df_final[\"Brain Related\"].mean()\n",
    "print(f\"Proportion brain-related: {prop_brain_related:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974fde63-420f-44e2-b669-79d782d33938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import json\n",
    "import os\n",
    "\n",
    "# there are two sampling strategies - randomly sample sentences or randomly sample reports\n",
    "# to prevent data leakage of models with context, and so forth, we randomly sample reports\n",
    "\n",
    "def split_sparse_by_report(sparse_csv, combined_csv, output_dir):\n",
    "    df_sparse = pd.read_csv(sparse_csv)\n",
    "    df_combined = pd.read_csv(combined_csv)\n",
    "    assert len(df_sparse) == len(df_combined), \"Mismatch between combined and sparse rows\"\n",
    "    df_sparse[\"File Path\"] = df_combined[\"File Path\"]\n",
    "    \n",
    "    df_sparse[\"report_name\"] = df_sparse[\"File Path\"].apply(lambda x: Path(x).stem)\n",
    "    \n",
    "    output_base = Path(output_dir)\n",
    "    output_base.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for report_name, group in df_sparse.groupby(\"report_name\"):\n",
    "        report_dir = output_base / report_name\n",
    "        report_dir.mkdir(parents=True, exist_ok=True)\n",
    "        group.drop(columns=[\"File Path\", \"report_name\"]).to_csv(report_dir / \"report.csv\", index=False)\n",
    "\n",
    "# train/test split at report level\n",
    "def get_train_test_indices_from_reports(base_dir, test_frac=0.2, seed=24):\n",
    "    np.random.seed(seed)\n",
    "    all_reports = sorted([p.name for p in Path(base_dir).iterdir() if p.is_dir()])\n",
    "    test_size = int(test_frac * len(all_reports))\n",
    "    test_reports = set(np.random.choice(all_reports, size=test_size, replace=False))\n",
    "    train_reports = [r for r in all_reports if r not in test_reports]\n",
    "    test_reports = list(test_reports)\n",
    "\n",
    "    train_report_frac = 1.3 # this is only for testing how models perform with less data\n",
    "    if train_report_frac < 1.0:\n",
    "        n_keep = int(train_report_frac * len(train_reports))\n",
    "        train_reports = list(np.random.choice(train_reports, size=n_keep, replace=False))\n",
    "        \n",
    "    with open(\"ml/standardized_train_report_names.json\", \"w\") as f:\n",
    "        json.dump(train_reports, f)\n",
    "    with open(\"ml/standardized_test_report_names.json\", \"w\") as f:\n",
    "        json.dump(test_reports, f)\n",
    "        \n",
    "    # ----------------------------------\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    row_counter = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    for report_name in all_reports:\n",
    "        csv_path = Path(base_dir) / report_name / \"report.csv\"\n",
    "        df       = pd.read_csv(csv_path)\n",
    "        n_rows   = len(df)\n",
    "\n",
    "        if report_name in test_reports:\n",
    "            test_indices .extend(range(row_counter, row_counter + n_rows))\n",
    "        elif report_name in train_reports:\n",
    "            train_indices.extend(range(row_counter, row_counter + n_rows))\n",
    "\n",
    "        row_counter += n_rows\n",
    "    \n",
    "    return np.array(train_indices), np.array(test_indices)\n",
    "\n",
    "\n",
    "# split reports\n",
    "output_dir = \"ml/sparse_individual\"\n",
    "if not os.path.exists(output_dir) or len(os.listdir(output_dir)) == 0:\n",
    "    split_sparse_by_report(\n",
    "        sparse_csv=\"ml/brain_sentences_sparse.csv\",\n",
    "        combined_csv=\"ml/brain_sentences_combined.csv\",\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    print(\"Split reports.\")\n",
    "else:\n",
    "    print(\"Skipping split: reports already split.\")\n",
    "\n",
    "SAMPLING_SENTENCES = False\n",
    "\n",
    "if not SAMPLING_SENTENCES:\n",
    "    # get train test split\n",
    "    train_idx_path = \"ml/standardized_train_indices_for_CRF_and_CLG.npy\"\n",
    "    test_idx_path  = \"ml/standardized_test_indices_for_CRF_and_CLG.npy\"\n",
    "\n",
    "    if os.path.exists(train_idx_path) and os.path.exists(test_idx_path):\n",
    "        train_indices = np.load(train_idx_path)\n",
    "        test_indices  = np.load(test_idx_path)\n",
    "        print(\"Number of train indices:\", len(train_indices))\n",
    "        print(\"Number of test indices:\", len(test_indices))\n",
    "        print(\"Loaded train/test indices from disk.\")\n",
    "    else:\n",
    "        train_indices, test_indices = get_train_test_indices_from_reports(\"ml/sparse_individual\", test_frac=0.2)\n",
    "        np.save(train_idx_path, train_indices)\n",
    "        np.save(test_idx_path, test_indices)\n",
    "        print(\"Number of train indices:\", len(train_indices))\n",
    "        print(\"Number of test indices:\", len(test_indices))\n",
    "        print(\"Randomized train/test split and saved indices.\")\n",
    "    \n",
    "    df_sparse = pd.read_csv(\"ml/brain_sentences_sparse.csv\")\n",
    "    df_combined = pd.read_csv(\"ml/brain_sentences_combined.csv\")\n",
    "    assert len(df_sparse) == len(df_combined), \"Mismatch between combined and sparse rows\"\n",
    "\n",
    "    df_sparse[\"File Path\"] = df_combined[\"File Path\"]  \n",
    "\n",
    "    y = df_sparse[\"Brain Related\"].values\n",
    "    X = df_sparse.drop(columns=[\"Brain Related\", \"File Path\"])  # drop for model input\n",
    "    X_sparse = csr_matrix(X.values)\n",
    "    \n",
    "    X_train = X.iloc[train_indices].reset_index(drop=True)\n",
    "    X_test = X.iloc[test_indices].reset_index(drop=True)\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    df_train_combined = df_combined.iloc[train_indices].reset_index(drop=True)\n",
    "    \n",
    "    # a possibility of how to train the metamodel in our contextual logistic regression approach but we do leave one out training instead\n",
    "    def delete_indices_for_local_subset(df_train, delete_frac=0.6, seed=24):\n",
    "        np.random.seed(seed)\n",
    "        df_train = df_train.reset_index(drop=True)\n",
    "        grouped = df_train.groupby(\"report_name\").indices\n",
    "        deleted = set()\n",
    "        total_candidates = sum(len(idxs) for idxs in grouped.values()) - len(grouped)\n",
    "        target_to_delete = int(delete_frac * total_candidates)\n",
    "        attempts = 0\n",
    "        max_attempts = 10 * target_to_delete\n",
    "\n",
    "        while len(deleted) < target_to_delete and attempts < max_attempts:\n",
    "            report = np.random.choice(list(grouped.keys()))\n",
    "            indices = grouped[report]\n",
    "            if len(indices) <= 1:\n",
    "                attempts += 1\n",
    "                continue\n",
    "            i = np.random.randint(0, len(indices) - 1)\n",
    "            idx_i = indices[i]\n",
    "            idx_ip1 = indices[i + 1]\n",
    "            if idx_i in deleted or idx_ip1 in deleted:\n",
    "                attempts += 1\n",
    "                continue\n",
    "            deleted.add(idx_i)\n",
    "            attempts += 1\n",
    "\n",
    "        all_indices = set(range(len(df_train)))\n",
    "        local_train_indices = sorted(all_indices - deleted)\n",
    "        deleted_indices = sorted(deleted)\n",
    "        return local_train_indices, deleted_indices\n",
    "\n",
    "    df_combined[\"report_name\"] = df_combined[\"File Path\"].apply(lambda x: Path(x).stem)\n",
    "    df_train_combined = df_combined.iloc[train_indices].reset_index(drop=True)\n",
    "    \n",
    "    local_train_indices, deleted_indices = delete_indices_for_local_subset(\n",
    "        df_train=df_train_combined,\n",
    "        delete_frac=0.0,\n",
    "        seed=24\n",
    "    )\n",
    "\n",
    "    print(\"Local model will train on\", len(local_train_indices), \"sentences.\")\n",
    "    print(\"This is because we have\", len(deleted_indices), \"sentences withheld from local model training.\")\n",
    "    print(\"The metamodel model will train on all\", len(local_train_indices)+len(deleted_indices), \"sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c435ae-86d8-4be2-8911-7994c5406d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all the feature engineering one wants and then do leave one fold out generation of predictions from the base model to avoid data leakage or overconfident trusting of the base model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "train_indices = np.load(\"ml/standardized_train_indices_for_CRF_and_CLG.npy\")\n",
    "test_indices = np.load(\"ml/standardized_test_indices_for_CRF_and_CLG.npy\")\n",
    "\n",
    "\n",
    "df_combined = pd.read_csv(\"ml/brain_sentences_combined.csv\")\n",
    "df_sparse = pd.read_csv(\"ml/brain_sentences_sparse.csv\")\n",
    "assert len(df_combined) == len(df_sparse)\n",
    "df_sparse[\"File Path\"] = df_combined[\"File Path\"]\n",
    "df_sparse[\"report_name\"] = df_sparse[\"File Path\"].apply(lambda x: Path(x).stem)\n",
    "df_sparse = df_sparse.sort_values(\n",
    "    \"report_name\",\n",
    "    kind=\"mergesort\", # stablesort\n",
    "    ignore_index=True\n",
    ")\n",
    "df_sparse.drop(columns=[\"report_name\"], inplace=True)\n",
    "\n",
    "OUT_OF_SCOPE_VALUE = 0.5 # we need to impute prior predicitons where you are one of the first few sentences and there is no prior. you can choose an unbiased prediction or shift it one way.\n",
    "\n",
    "DO_PAIRWISE_BOW_INTERACTIONS = False \n",
    "if DO_PAIRWISE_BOW_INTERACTIONS:\n",
    "    from itertools import combinations\n",
    "\n",
    "    # select bow columns eligible for pairwise interaction\n",
    "    exclude_patterns = [\"Prior_\", \"_x_\", \"Facial_Bones\", \"spine_earlier\", \"Sent_Num\", \"Brain Related\"]\n",
    "    def is_bow(col):\n",
    "        return all(pat not in col for pat in exclude_patterns)\n",
    "\n",
    "    bow_cols = [col for col in df_sparse.columns\n",
    "                if is_bow(col)\n",
    "                and np.issubdtype(df_sparse.dtypes[col], np.number)]\n",
    "    \n",
    "    # filter bow columns to those with at least 30 instances to restrict the size of the combination\n",
    "    bow_cols = [col for col in bow_cols if df_sparse[col].sum() >= 30]\n",
    "    \n",
    "    print(f\"Eligible BoW columns for pairwise interaction: {len(bow_cols)}\")\n",
    "    \n",
    "    # compute all unique (unordered) pairwise interactions\n",
    "    pairwise_interactions = {}\n",
    "    for col1, col2 in combinations(bow_cols, 2):\n",
    "        inter_col = f\"BOWPAIR_{col1}_x_{col2}\"\n",
    "        pairwise_interactions[inter_col] = df_sparse[col1] * df_sparse[col2]\n",
    "    \n",
    "    print(f\"Total pairwise interactions computed: {len(pairwise_interactions)}\")\n",
    "    \n",
    "    if pairwise_interactions:\n",
    "        pairwise_df = pd.DataFrame(pairwise_interactions, index=df_sparse.index)\n",
    "    \n",
    "        interaction_freq = pairwise_df.sum(axis=0)\n",
    "    \n",
    "        # select the top 300 most frequent interaction columns\n",
    "        top_300_inter_cols = interaction_freq.sort_values(ascending=False).head(300).index\n",
    "    \n",
    "        pairwise_df = pairwise_df[top_300_inter_cols]\n",
    "    \n",
    "        df_sparse = pd.concat([df_sparse, pairwise_df], axis=1)\n",
    "    \n",
    "        print(f\"Added {len(pairwise_df.columns)} most frequent pairwise BoW interaction columns.\") \n",
    "    else:\n",
    "        print(\"No eligible pairwise BoW interactions to add.\")\n",
    "\n",
    "# optionally, like the spine features, do a similar thing for facial bones (interactions and past flag)\n",
    "DO_FACIAL_BONES_EARLIER = False\n",
    "FACIAL_BONES_INTERACTION = False\n",
    "if DO_FACIAL_BONES_EARLIER:\n",
    "    # Add 'Sentence' for lookup\n",
    "    df_sentences = pd.read_csv(\"ml/brain_sentences_combined.csv\")\n",
    "    df_sparse[\"Sentence\"] = df_sentences[\"Sentence\"]\n",
    "    \n",
    "    def has_facial_bones(prior_sentences):\n",
    "        pattern = re.compile(r\"(facial bones|_ial bones)\", re.IGNORECASE)\n",
    "        return any(pattern.search(s) for s in prior_sentences)\n",
    "    \n",
    "    facial_bones_earlier = []\n",
    "    for idx in range(len(df_sparse)):\n",
    "        current_file = df_sparse.at[idx, \"File Path\"]\n",
    "        prior_indices = [i for i in range(idx) if df_sparse.at[i, \"File Path\"] == current_file]\n",
    "        prior_sents = df_sparse.loc[prior_indices, \"Sentence\"].tolist() if prior_indices else []\n",
    "        facial_bones_earlier.append(1 if has_facial_bones(prior_sents) else 0)\n",
    "    df_sparse[\"Facial_Bones_earlier\"] = facial_bones_earlier\n",
    "\n",
    "    df_sparse.drop(columns=[\"Sentence\"], inplace=True) # sentence is not sparse\n",
    "    \n",
    "    if FACIAL_BONES_INTERACTION:\n",
    "        exclude_substrings = [\"_x_spine\", \"_x_Prior1\"]\n",
    "        eligible_cols = [\n",
    "            col for col in df_sparse.columns\n",
    "            if np.issubdtype(df_sparse[col].dtype, np.number)\n",
    "            and all(substr not in col for substr in exclude_substrings)\n",
    "            and col not in [\"Facial_Bones_earlier\", \"Brain Related\"]  # exclude label\n",
    "        ]\n",
    "\n",
    "        interaction_df = pd.DataFrame({\n",
    "            f\"{col}_x_FacialBones\": df_sparse[col] * df_sparse[\"Facial_Bones_earlier\"]\n",
    "            for col in eligible_cols\n",
    "        }, index=df_sparse.index)\n",
    "        df_sparse = pd.concat([df_sparse, interaction_df], axis=1)\n",
    "\n",
    "\n",
    "y = df_sparse[\"Brain Related\"].values\n",
    "X = df_sparse.drop(columns=[\"Brain Related\", \"File Path\"])\n",
    "\n",
    "\n",
    "cols_to_zero = X.filter(regex=r'(?i)spine_earlier|sent[\\s_-]*num').columns\n",
    "print(f\"Zeroing {len(cols_to_zero)} columns:\", list(cols_to_zero))\n",
    "X.loc[:, cols_to_zero] = 0\n",
    "\n",
    "X.to_parquet(\"ml/extended_base_X.parquet\", index=False)\n",
    "np.save(\"ml/extended_base_y.npy\", y)\n",
    "\n",
    "# depending on whether you want these or not\n",
    "cols_to_zero = X.filter(regex=r'(?i)spine_earlier|sent[\\s_-]*num').columns\n",
    "print(f\"Zeroing {len(cols_to_zero)} columns:\", list(cols_to_zero))\n",
    "X.loc[:, cols_to_zero] = 0\n",
    "\n",
    "\n",
    "BASE_MODEL_C = 1.0\n",
    "\n",
    "\n",
    "unique_paths = df_sparse.loc[train_indices, \"File Path\"].unique()\n",
    "print(f\"Total train reports: {len(unique_paths)}\")\n",
    "\n",
    "# decide how many reports you want to keep:\n",
    "p = 1.0   # e.g. keep 50% or 100% of your train reports to train on less data\n",
    "n_keep = int(p * len(unique_paths))\n",
    "\n",
    "if p < 1.0:\n",
    "    # randomly choose that many reports\n",
    "    np.random.seed(24)\n",
    "    selected_paths = np.random.choice(unique_paths, size=n_keep, replace=False)\n",
    "    print(f\"Keeping {n_keep} reports:\", selected_paths[:5], \"...\")\n",
    "    \n",
    "    # new train set after filtering out\n",
    "    train_indices = np.array([\n",
    "        idx for idx in train_indices\n",
    "        if df_sparse.at[idx, \"File Path\"] in selected_paths\n",
    "    ])\n",
    "    \n",
    "    print(\"New train size (sentences):\", len(train_indices))\n",
    "    print(\"Unique reports now:\", \n",
    "          np.unique(df_sparse.loc[train_indices, \"File Path\"]).shape[0])\n",
    "\n",
    "loo_preds = np.full(len(df_sparse), np.nan)\n",
    "\n",
    "LOO_K = 50  # Set to 1 for leave one out, >1 for leave-k-out in blocks for speed\n",
    "if LOO_K == 1:\n",
    "    # standard Leave one out\n",
    "    for idx in tqdm(train_indices, desc=\"LOO on train set\"):\n",
    "        train_loo = [i for i in train_indices if i != idx]\n",
    "        model = LogisticRegression(C=BASE_MODEL_C, max_iter=10000, penalty=\"l2\", solver=\"saga\", fit_intercept=False)\n",
    "        model.fit(X.iloc[train_loo], y[train_loo])\n",
    "        loo_preds[idx] = model.predict_proba(X.iloc[[idx]])[:, 1][0]\n",
    "else:\n",
    "    # leave-k-out in blocks (folds)\n",
    "    train_indices_sorted = np.sort(train_indices)\n",
    "    n_train = len(train_indices_sorted)\n",
    "    for i in tqdm(range(0, n_train, LOO_K), desc=f\"Leave-{LOO_K}-out in blocks\"):\n",
    "        block = train_indices_sorted[i:i+LOO_K]\n",
    "        train_loo = [j for j in train_indices_sorted if j not in block]\n",
    "        model = LogisticRegression(C=BASE_MODEL_C, max_iter=10000, penalty=\"l2\", solver=\"saga\", fit_intercept=False)\n",
    "        model.fit(X.iloc[train_loo], y[train_loo])\n",
    "        loo_preds[block] = model.predict_proba(X.iloc[block])[:, 1]\n",
    "\n",
    "# predict test set using base model trained on full train set ---\n",
    "base_model = LogisticRegression(C=BASE_MODEL_C, max_iter=10000, penalty=\"l2\", solver=\"saga\", fit_intercept=False)\n",
    "base_model.fit(X.iloc[train_indices], y[train_indices])\n",
    "for idx in tqdm(test_indices, desc=\"Predicting test set\"):\n",
    "    loo_preds[idx] = base_model.predict_proba(X.iloc[[idx]])[:, 1][0]\n",
    "\n",
    "df_sparse[\"Model_Prediction\"] = loo_preds\n",
    "\n",
    "# compute prior features \n",
    "df_sparse[\"Prior_1_Prediction\"] = OUT_OF_SCOPE_VALUE\n",
    "for i in range(len(df_sparse)):\n",
    "    current_file = df_sparse.at[i, \"File Path\"]\n",
    "    if i >= 1:\n",
    "        prev_file = df_sparse.at[i - 1, \"File Path\"]\n",
    "        if current_file == prev_file:\n",
    "            df_sparse.at[i, \"Prior_1_Prediction\"] = float(df_sparse.at[i - 1, \"Model_Prediction\"])\n",
    "        else:\n",
    "            df_sparse.at[i, \"Prior_1_Prediction\"] = float(OUT_OF_SCOPE_VALUE)\n",
    "    else:\n",
    "        df_sparse.at[i, \"Prior_1_Prediction\"] = float(OUT_OF_SCOPE_VALUE)\n",
    "\n",
    "# shift for neutral to be 0 for interpretability\n",
    "df_sparse[\"Prior_1_Prediction\"] = df_sparse[\"Prior_1_Prediction\"].astype(float)\n",
    "df_sparse[\"Prior_1_Prediction\"] -= 0.5\n",
    "\n",
    "df_sparse.to_csv(\"ml/brain_sentences_sparse_meta_for_additive_logit.csv\", index=False, float_format=\"%.6f\")\n",
    "df_sparse.drop(columns=[\"Model_Prediction\", \"File Path\"], inplace=True)\n",
    "df_sparse.to_csv(\"ml/brain_sentences_sparse_meta.csv\", index=False, float_format=\"%.6f\")\n",
    "print(\"Updated dataset saved with leakage-free prior predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d35247-f43d-4a82-88f4-b3e9281724de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meteamodel training and performance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"ml/brain_sentences_sparse_meta_for_additive_logit.csv\")\n",
    "\n",
    "OUT_OF_SCOPE_VALUE = 0.5 \n",
    "\n",
    "# compute shifted prior predictions for the previous x sentences\n",
    "HOW_MUCH_HISTORY = 4 # how many previous\n",
    "for k in range(1, HOW_MUCH_HISTORY+1):\n",
    "    col_name = f\"Prior_{k}_Prediction\"\n",
    "    df[col_name] = OUT_OF_SCOPE_VALUE  # initialize\n",
    "    for i in range(len(df)):\n",
    "        current_file = df.at[i, \"File Path\"]\n",
    "        if i >= k:\n",
    "            prev_file = df.at[i - k, \"File Path\"]\n",
    "            if current_file == prev_file:\n",
    "                df.at[i, col_name] = float(df.at[i - k, \"Model_Prediction\"])\n",
    "            else:\n",
    "                df.at[i, col_name] = float(OUT_OF_SCOPE_VALUE)\n",
    "        else:\n",
    "            df.at[i, col_name] = float(OUT_OF_SCOPE_VALUE)\n",
    "    # shift as before\n",
    "    df[col_name] = df[col_name].astype(float) - 0.5\n",
    "\n",
    "# compute the aggregation features (mean, max over Prior 1/2/3 shifted)\n",
    "df[\"Prior_1234on_Avg\"] = df[[f\"Prior_{k}_Prediction\" for k in range(1, HOW_MUCH_HISTORY+1)]].mean(axis=1)\n",
    "df[\"Prior_1234on_Max\"] = df[[f\"Prior_{k}_Prediction\" for k in range(1, HOW_MUCH_HISTORY+1)]].max(axis=1)\n",
    "df[\"Prior_1234on_Min\"] = df[[f\"Prior_{k}_Prediction\" for k in range(1, HOW_MUCH_HISTORY+1)]].min(axis=1)\n",
    "#df.drop(columns=[\"Prior_2_Prediction\", \"Prior_3_Prediction\"], inplace=True)\n",
    "df.drop(columns=[\"Model_Prediction\"], inplace=True)\n",
    "\n",
    "# df = df.drop(columns=[c for c in df.columns if c.startswith(\"Prior_\")]) \n",
    "\n",
    "# repeated code if you wanted to do interactions for the metamodel and not the base model, etc\n",
    "from itertools import combinations\n",
    "DO_PAIRWISE_INTERACTIONS = False \n",
    "if DO_PAIRWISE_INTERACTIONS:\n",
    "    exclude_patterns = [\"Prior_\", \"_x_\", \"Facial_Bones\", \"spine_earlier\", \"Sent_Num\", \"Brain Related\"]\n",
    "    def is_bow(col):\n",
    "        return all(pat not in col for pat in exclude_patterns)\n",
    "    \n",
    "    bow_cols = [col for col in df.columns\n",
    "                if is_bow(col)\n",
    "                and np.issubdtype(df.dtypes[col], np.number)]\n",
    "    \n",
    "    bow_cols = [col for col in bow_cols if df.loc[train_indices, col].sum() >= 15]\n",
    "    \n",
    "    print(f\"Eligible BoW columns for pairwise interaction: {len(bow_cols)}\")\n",
    "    \n",
    "    pairwise_interactions = {}\n",
    "    for col1, col2 in combinations(bow_cols, 2):\n",
    "        inter_col = f\"{col1}_x_{col2}\"\n",
    "        pairwise_interactions[inter_col] = df[col1] * df[col2]\n",
    "    \n",
    "    print(f\"Total pairwise interactions computed: {len(pairwise_interactions)}\")\n",
    "    \n",
    "    if pairwise_interactions:\n",
    "        pairwise_df = pd.DataFrame(pairwise_interactions, index=df.index)\n",
    "    \n",
    "        interaction_freq = pairwise_df.loc[train_indices].sum(axis=0)\n",
    "    \n",
    "        top_300_inter_cols = interaction_freq.sort_values(ascending=False).head(1000).index\n",
    "    \n",
    "        pairwise_df = pairwise_df[top_300_inter_cols]\n",
    "    \n",
    "        df = pd.concat([df, pairwise_df], axis=1)\n",
    "    \n",
    "        print(f\"Added {len(pairwise_df.columns)} most frequent pairwise BoW interaction columns.\")\n",
    "\n",
    "        top_300_inter_cols = interaction_freq.sort_values(ascending=False).head(300).index.tolist()\n",
    "        pd.Series(top_300_inter_cols).to_csv(\"ml/top_bow_pairwise_cols.csv\", index=False)\n",
    "    \n",
    "    else:\n",
    "        print(\"No eligible pairwise BoW interactions to add.\")\n",
    "\n",
    "\n",
    "PRIOR_INTERACTION_MODE = 1  # 1: none, 2: exclude spine interactions, 3: all\n",
    "exclude_cols = [col for col in df.columns if col.startswith(\"Prior_\")] + [\"Brain Related\"]\n",
    "\n",
    "if PRIOR_INTERACTION_MODE == 1:\n",
    "    numeric_cols = []\n",
    "elif PRIOR_INTERACTION_MODE == 2:\n",
    "    numeric_cols = [col for col in df.columns\n",
    "                if col not in exclude_cols\n",
    "                and np.issubdtype(df.dtypes[col], np.number)\n",
    "                and \"_x_spine\" not in col]\n",
    "elif PRIOR_INTERACTION_MODE == 3:\n",
    "   numeric_cols = [col for col in df.columns\n",
    "                if col not in exclude_cols\n",
    "                and np.issubdtype(df.dtypes[col], np.number)]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"PRIOR_INTERACTION_MODE must be 1, 2, or 3\")\n",
    "\n",
    "# only make interaction terms if needed\n",
    "if numeric_cols:\n",
    "    prior1 = df[\"Prior_1_Prediction\"]\n",
    "    interaction_data = {f\"{col}_x_Prior1\": df[col] * prior1 for col in numeric_cols}\n",
    "    interactions_df = pd.DataFrame(interaction_data, index=df.index)\n",
    "    df = pd.concat([df, interactions_df], axis=1)\n",
    "\n",
    "\n",
    "# these can be false if you did it for the base model because you'd inherit it\n",
    "DO_FACIAL_BONES_EARLIER = False\n",
    "FACIAL_BONES_INTERACTION = False\n",
    "import re\n",
    "if DO_FACIAL_BONES_EARLIER:\n",
    "    def has_facial_bones(prior_sentences):\n",
    "        pattern = re.compile(r\"(facial bones|_ial bones)\", re.IGNORECASE)\n",
    "        return any(pattern.search(s) for s in prior_sentences)\n",
    "    \n",
    "    facial_bones_earlier = []\n",
    "    \n",
    "    df_sentences = pd.read_csv(\"ml/brain_sentences_combined.csv\")\n",
    "    \n",
    "    df_sentences[\"report_name\"] = df_sentences[\"File Path\"].apply(lambda fp: Path(fp).stem)\n",
    "    df_sentences.sort_values(\"report_name\", inplace=True, kind=\"mergesort\", ignore_index=True)\n",
    "    df_sentences.drop(columns=[\"report_name\"], inplace=True)\n",
    "\n",
    "    df[\"Sentence\"] = df_sentences[\"Sentence\"]\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        current_file = df.at[idx, \"File Path\"]\n",
    "        # find all prior sentences from the same report\n",
    "        prior_indices = [i for i in range(idx) if df.at[i, \"File Path\"] == current_file]\n",
    "        prior_sents = df.loc[prior_indices, \"Sentence\"].tolist() if prior_indices else []\n",
    "        facial_bones_earlier.append(1 if has_facial_bones(prior_sents) else 0)\n",
    "        \n",
    "    df[\"Facial_Bones_earlier\"] = facial_bones_earlier\n",
    "    df.drop(columns=[\"Sentence\"], inplace=True)\n",
    "    #print(\"Number of sentences with prior 'Facial Bones' mention:\", df[\"Facial_Bones_earlier\"].sum())\n",
    "\n",
    "    if FACIAL_BONES_INTERACTION:\n",
    "        exclude_substrings = [\"_x_spine\", \"_x_Prior1\"]\n",
    "        eligible_cols = [col for col in df.columns\n",
    "                         if np.issubdtype(df[col].dtype, np.number)\n",
    "                         and all(substr not in col for substr in exclude_substrings)\n",
    "                         and col not in [\"Facial_Bones_earlier\", \"Brain Related\"]] # label shouldn't be in here but just in case\n",
    "\n",
    "        interaction_df = pd.DataFrame({\n",
    "            f\"{col}_x_FacialBones\": df[col] * df[\"Facial_Bones_earlier\"]\n",
    "            for col in eligible_cols\n",
    "        }, index=df.index)\n",
    "        df = pd.concat([df, interaction_df], axis=1)\n",
    "\n",
    "\n",
    "df.to_csv(\"ml/prep_for_classical_crf.csv\", index=False, float_format=\"%.6f\")\n",
    "\n",
    "df.drop(columns=[\"File Path\"], inplace=True)\n",
    "\n",
    "df.to_csv(\"ml/brain_sentences_sparse_meta_more_aggregation.csv\", index=False, float_format=\"%.6f\")\n",
    "print(\"Saved to ml/brain_sentences_sparse_meta_more_aggregation.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "PRIOR_GENERALIZATION_SCALE_FACTOR = 1.0\n",
    "PRIOR_SCALE_BEFORE_TO_GET_REG_APPLIED = 1.0\n",
    "SPINE_FEATURE_SCALE = 1.0\n",
    "PRIOR_INTERACTION_FEATURE_SCALE = 1.0\n",
    "FACIAL_BONES_INTERACTION_SCALE = 1.0\n",
    "CHOSEN_C = 1.0\n",
    "\n",
    "df = pd.read_csv(\"ml/brain_sentences_sparse_meta_more_aggregation.csv\")\n",
    "\n",
    "y = df[\"Brain Related\"].values\n",
    "X = df.drop(columns=[\"Brain Related\"])\n",
    "X.index = df.index\n",
    "\n",
    "cols_to_zero = X.filter(regex=r'(?i)spine_earlier|sent[\\s_\\-]*num').columns\n",
    "X.loc[:, cols_to_zero] = 0\n",
    "\n",
    "# optional scaling to enforce regularization differently\n",
    "prior_cols = [col for col in X.columns if col.startswith(\"Prior_\")]\n",
    "X[prior_cols] = X[prior_cols] * PRIOR_SCALE_BEFORE_TO_GET_REG_APPLIED\n",
    " \n",
    "spine_cols = [col for col in X.columns if '_x_spine' in col]\n",
    "X[spine_cols] = X[spine_cols] * SPINE_FEATURE_SCALE\n",
    "\n",
    "prior1_interaction_cols = [col for col in X.columns if col.endswith('_x_Prior1')]\n",
    "X[prior1_interaction_cols] = X[prior1_interaction_cols] * PRIOR_INTERACTION_FEATURE_SCALE\n",
    "\n",
    "facial_bones_interaction_cols = [col for col in X.columns if col.endswith('_x_FacialBones')]\n",
    "X[facial_bones_interaction_cols] = X[facial_bones_interaction_cols] * FACIAL_BONES_INTERACTION_SCALE\n",
    "\n",
    "X_train = X.iloc[train_indices].copy()\n",
    "X_test = X.iloc[test_indices].copy()\n",
    "y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "for col in prior_cols:\n",
    "    if col in X_test.columns:\n",
    "        X_test[col] *= PRIOR_GENERALIZATION_SCALE_FACTOR\n",
    "\n",
    "# train metamodel\n",
    "model = LogisticRegression(\n",
    "    max_iter=10000, penalty=\"l2\", C=CHOSEN_C, solver=\"liblinear\", fit_intercept=False\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nTest Set Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# save the metamodel\n",
    "import joblib\n",
    "joblib.dump(model, \"ml/logistic_regression_brain_related_meta_more_aggregation.pkl\")\n",
    "print(\"Meta model saved to ml/logistic_regression_brain_related_meta_more_aggregation.pkl\")\n",
    "\n",
    "if True:\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    cm_test = cm\n",
    "\n",
    "    if cm_test.shape == (2, 2):\n",
    "        TN = cm_test[0, 0]\n",
    "        FP = cm_test[0, 1]\n",
    "        FN = cm_test[1, 0]\n",
    "        TP = cm_test[1, 1]\n",
    "\n",
    "        recall_1 = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        recall_0 = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "\n",
    "        total = TN + FP + FN + TP\n",
    "        actual_prop_0 = (TN + FP) / total\n",
    "        actual_prop_1 = (TP + FN) / total\n",
    "        print(f\"Test set distribution: class 0 = {actual_prop_0:.2%}, class 1 = {actual_prop_1:.2%}\")\n",
    "\n",
    "        for target_prop_0, target_prop_1 in [\n",
    "            (0.10, 0.90),\n",
    "            (0.15, 0.85),\n",
    "            (0.50, 0.50),\n",
    "            (0.90, 0.10),\n",
    "            (actual_prop_0, actual_prop_1)\n",
    "        ]:\n",
    "            weighted_acc = target_prop_1 * recall_1 + target_prop_0 * recall_0\n",
    "            print(f\"Weighted accuracy ({target_prop_1:.0%} class 1, {target_prop_0:.0%} class 0): {weighted_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"Unexpected confusion-matrix shape:\", cm_test.shape)\n",
    "\n",
    "    # reverse the order so that class 1 appears first\n",
    "    cm = cm[::-1, ::-1]\n",
    "\n",
    "    class_labels = [\"Brain Related\", \"Not Brain Related\"]\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    joblib.dump(model, \"ml/logistic_regression_brain_related.pkl\")\n",
    "    print(\"Model trained and saved as ml/logistic_regression_brain_related.pkl.\")\n",
    "\n",
    "    feature_names = X.columns\n",
    "    coefs = model.coef_[0]\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coefs\n",
    "    })\n",
    "\n",
    "    # compute absolute coefficients to rank by importance\n",
    "    coef_df['abs_coef'] = coef_df['coefficient'].abs()\n",
    "\n",
    "    # Sort by the absolute value of coefficients in descending order and select the top x\n",
    "    topx = coef_df.sort_values(by='abs_coef', ascending=False).head(40)\n",
    "\n",
    "    # determine the bar colors: blue if the coefficient is positive, red if negative\n",
    "    colors = topx['coefficient'].apply(lambda x: 'blue' if x > 0 else 'red')\n",
    "\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    plt.barh(topx['feature'][::-1], topx['coefficient'][::-1], color=colors[::-1])\n",
    "    plt.xlabel(\"Coefficient Value\")\n",
    "    plt.title(\"Top Features by Coefficient Magnitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    y_scores = model.decision_function(X_test)  \n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    top25 = coef_df.sort_values(by='abs_coef', ascending=False).head(45)\n",
    "    \n",
    "    print(\"Top 25 features by absolute coefficient magnitude:\")\n",
    "    print(top25[['feature', 'coefficient']].to_string(index=False))\n",
    "\n",
    "    from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "    meta_probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "    meta_results = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        meta_preds = (meta_probs > thresh).astype(int)\n",
    "        acc = accuracy_score(y_test, meta_preds)\n",
    "        f1_cls1 = f1_score(y_test, meta_preds, pos_label=1, zero_division=0)\n",
    "        f1_macro = f1_score(y_test, meta_preds, average=\"macro\", zero_division=0)  # NEW\n",
    "        meta_results.append((thresh, f1_cls1, f1_macro, acc))\n",
    "        print(f\"Threshold {thresh:.2f}: Accuracy={acc:.4f}, F1_1={f1_cls1:.4f}, MacroF1={f1_macro:.4f}\")\n",
    "    \n",
    "    meta_df = pd.DataFrame(meta_results, columns=[\"threshold\", \"F1_class1\", \"MacroF1\", \"accuracy\"])\n",
    "    meta_df.to_csv(\"meta_threshold_f1_acc.csv\", index=False)\n",
    "    print(\"Saved meta model threshold metrics (F1_1, MacroF1, accuracy) to meta_threshold_f1_acc.csv\")\n",
    "    \n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(meta_df[\"threshold\"], meta_df[\"accuracy\"], marker='o', label='Accuracy')\n",
    "    plt.plot(meta_df[\"threshold\"], meta_df[\"F1_class1\"], marker='s', label='F1 (Class 1)')\n",
    "    plt.plot(meta_df[\"threshold\"], meta_df[\"MacroF1\"], marker='^', label='Macro F1')  # NEW\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Meta Model: Accuracy, F1 (Class 1) & Macro F1 vs Threshold')\n",
    "    plt.legend()\n",
    "    plt.xlim(0, 0.99)\n",
    "    plt.ylim(0.8, 1)\n",
    "    plt.grid(True, alpha=0.4)\n",
    "    plt.show()\n",
    "    \n",
    "np.save(\"ml/global_model_probabilities.npy\", meta_probs.astype(np.float32))\n",
    "np.save(\"ml/global_model_labels.npy\",      y_test.astype(np.int32))\n",
    "np.savez(\"ml/global_model_test_scores_labels.npz\",\n",
    "         scores=meta_probs.astype(np.float32),\n",
    "         labels=y_test.astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ade8a9-a55f-48cf-8665-be33e3f0ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classical crf\n",
    "import pandas as pd\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from pathlib import Path\n",
    "\n",
    "df = pd.read_csv(\"ml/prep_for_classical_crf.csv\")\n",
    "\n",
    "# drop columns containing 'Prior', crf has its transition probabilities\n",
    "prior_cols = [col for col in df.columns if \"Prior\" in col]\n",
    "df = df.drop(columns=prior_cols)\n",
    "\n",
    "df[\"report_name\"] = df[\"File Path\"].apply(lambda fp: Path(fp).stem)\n",
    "df.sort_values(\n",
    "    \"report_name\",\n",
    "    kind=\"mergesort\", \n",
    "    inplace=True,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "zero_cols = df.filter(regex=r'(?i)spine_earlier|sent[\\s_\\-]*num').columns\n",
    "print(f\"Zeroing {len(zero_cols)} CRF columns:\", list(zero_cols)[:10], \"...\" if len(zero_cols) > 10 else \"\")\n",
    "df.loc[:, zero_cols] = 0\n",
    "\n",
    "df[\"Split\"] = \"unused\"\n",
    "df.loc[train_indices, \"Split\"] = \"train\"\n",
    "df.loc[test_indices,  \"Split\"] = \"test\"\n",
    "\n",
    "def row_to_feats(row):\n",
    "    return {\n",
    "        col: row[col]\n",
    "        for col in df.columns\n",
    "        if col not in (\"File Path\", \"report_name\", \"Brain Related\", \"Split\")\n",
    "    }\n",
    "\n",
    "X_train, y_train = [], []\n",
    "X_test,  y_test  = [], []\n",
    "\n",
    "for report, group in df.groupby(\"report_name\"):\n",
    "    split = group[\"Split\"].iloc[0]\n",
    "    # skip any “unused” reports - this is if we train on less data\n",
    "    if split not in (\"train\", \"test\"):\n",
    "        continue\n",
    "\n",
    "    X_seq = [row_to_feats(r) for _, r in group.iterrows()]\n",
    "    y_seq = group[\"Brain Related\"].astype(str).tolist()\n",
    "\n",
    "    if split == \"train\":\n",
    "        X_train.append(X_seq)\n",
    "        y_train.append(y_seq)\n",
    "    else:\n",
    "        X_test.append(X_seq)\n",
    "        y_test.append(y_seq)\n",
    "\n",
    "\n",
    "train_report_names = sorted(df.loc[df[\"Split\"] == \"train\", \"report_name\"].unique())\n",
    "test_report_names  = sorted(df.loc[df[\"Split\"] == \"test\",  \"report_name\"].unique())\n",
    "\n",
    "with open(\"ml/from_classical_standardized_train_report_names.json\", \"w\") as f:\n",
    "    json.dump(train_report_names, f)\n",
    "with open(\"ml/from_classical_standardized_test_report_names.json\", \"w\") as f:\n",
    "    json.dump(test_report_names, f)\n",
    "\n",
    "print(\"Saved standardized train/test report names for CRF compatibility.\")\n",
    "\n",
    "df.drop(columns=[\"report_name\"], inplace=True)\n",
    "\n",
    "\n",
    "crf = CRF(\n",
    "    algorithm=\"lbfgs\",\n",
    "    c1=1.5,\n",
    "    c2=2.0,\n",
    "    max_iterations=10000,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "print(flat_classification_report(\n",
    "    y_test, y_pred, labels=[\"0\", \"1\"], digits=4,\n",
    "    target_names=[\"Not Brain‑Related\", \"Brain‑Related\"]\n",
    "))\n",
    "\n",
    "y_test_flat = [item for seq in y_test for item in seq]\n",
    "y_pred_flat = [item for seq in y_pred for item in seq]\n",
    "\n",
    "acc = accuracy_score(y_test_flat, y_pred_flat)\n",
    "print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "\n",
    "bacc = balanced_accuracy_score(y_test_flat, y_pred_flat)\n",
    "print(f\"Balanced Accuracy (macro recall): {bacc:.4f}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_test = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "\n",
    "if cm_test.shape == (2, 2):\n",
    "    TN = cm_test[0, 0]\n",
    "    FP = cm_test[0, 1]\n",
    "    FN = cm_test[1, 0]\n",
    "    TP = cm_test[1, 1]\n",
    "\n",
    "    recall_1 = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    recall_0 = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "\n",
    "    total = TN + FP + FN + TP\n",
    "    actual_prop_0 = (TN + FP) / total\n",
    "    actual_prop_1 = (TP + FN) / total\n",
    "    print(f\"Test set distribution: class 0 = {actual_prop_0:.2%}, class 1 = {actual_prop_1:.2%}\")\n",
    "\n",
    "    for target_prop_0, target_prop_1 in [\n",
    "        (0.10, 0.90),\n",
    "        (0.15, 0.85),\n",
    "        (0.50, 0.50),\n",
    "        (0.90, 0.10),\n",
    "        (actual_prop_0, actual_prop_1)\n",
    "    ]:\n",
    "        weighted_acc = target_prop_1 * recall_1 + target_prop_0 * recall_0\n",
    "        print(f\"Weighted accuracy ({target_prop_1:.0%} class 1, {target_prop_0:.0%} class 0): {weighted_acc:.4f}\")\n",
    "else:\n",
    "    print(\"Unexpected confusion-matrix shape:\", cm_test.shape)\n",
    "\n",
    "per_report_accuracies = []\n",
    "for y_true_seq, y_pred_seq in zip(y_test, y_pred):\n",
    "    if len(y_true_seq) == 0:\n",
    "        continue\n",
    "    n_correct = sum(int(t == p) for t, p in zip(y_true_seq, y_pred_seq))\n",
    "    acc = n_correct / len(y_true_seq)\n",
    "    per_report_accuracies.append(acc)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(per_report_accuracies, bins=15, color='dodgerblue', alpha=0.8)\n",
    "plt.xlabel(\"Per-Report Accuracy\")\n",
    "plt.ylabel(\"Number of Reports\")\n",
    "plt.title(\"Histogram of CRF Accuracy per Test Report\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get CRF marginal probabilities for each sentence in the test set\n",
    "probs_flat = []\n",
    "labels_flat = []\n",
    "\n",
    "for X_seq, y_seq in zip(X_test, y_test):\n",
    "    marginals = crf.predict_marginals_single(X_seq)\n",
    "    for marg, label in zip(marginals, y_seq):\n",
    "        probs_flat.append(marg[\"1\"])\n",
    "        labels_flat.append(int(label))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(labels_flat, probs_flat)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=f'CRF ROC (AUC = {roc_auc:.4f})', lw=2)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"CRF ROC Curve (Sentence-Level)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "crf_results = []\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "crf_results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    preds_at_thresh = (np.array(probs_flat) > thresh).astype(int)\n",
    "    acc = accuracy_score(labels_flat, preds_at_thresh)\n",
    "    f1_cls1 = f1_score(labels_flat, preds_at_thresh, pos_label=1, zero_division=0)\n",
    "    f1_macro = f1_score(labels_flat, preds_at_thresh, average=\"macro\", zero_division=0)  # NEW\n",
    "    crf_results.append((thresh, f1_cls1, f1_macro, acc))\n",
    "    print(f\"Threshold {thresh:.2f}: Accuracy={acc:.4f}, F1_1={f1_cls1:.4f}, MacroF1={f1_macro:.4f}\")\n",
    "\n",
    "crf_df = pd.DataFrame(crf_results, columns=[\"threshold\", \"F1_class1\", \"MacroF1\", \"accuracy\"])\n",
    "crf_df.to_csv(\"crf_threshold_f1_acc.csv\", index=False)\n",
    "print(\"Saved CRF threshold metrics (F1_1, MacroF1, accuracy) to crf_threshold_f1_acc.csv\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(crf_df[\"threshold\"], crf_df[\"accuracy\"], marker='o', label='Accuracy')\n",
    "plt.plot(crf_df[\"threshold\"], crf_df[\"F1_class1\"], marker='s', label='F1 (Class 1)')\n",
    "plt.plot(crf_df[\"threshold\"], crf_df[\"MacroF1\"], marker='^', label='Macro F1')  # NEW\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('CRF: Accuracy, F1 (Class 1) & Macro F1 vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.xlim(0, 0.99)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.show()\n",
    "\n",
    "probs_arr  = np.asarray(probs_flat,  dtype=np.float32)\n",
    "labels_arr = np.asarray(labels_flat, dtype=np.int32)\n",
    "\n",
    "np.save(\"ml/crf_model_probabilities.npy\", probs_arr)\n",
    "np.save(\"ml/crf_model_labels.npy\",       labels_arr)\n",
    "np.savez(\"ml/classical_crf_test_scores_labels.npz\",\n",
    "         scores=probs_arr, labels=labels_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8eccc-5da0-40cc-9ffd-9b02cd1020fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basically the early code but varying the dataset size, etc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "train_indices = np.load(\"ml/standardized_train_indices_for_CRF_and_CLG.npy\")\n",
    "test_indices = np.load(\"ml/standardized_test_indices_for_CRF_and_CLG.npy\")\n",
    "\n",
    "p_s = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9, 1.0]\n",
    "dataset_sample_seeds = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "\n",
    "\n",
    "# will collect one row per (p, seed)\n",
    "results = []\n",
    "\n",
    "for p in p_s:\n",
    "    for s in dataset_sample_seeds:\n",
    "        if s>1 and p >= 1: continue\n",
    "        print(f\"Processing with p = {p}, seed = {s}\")\n",
    "        train_indices = np.load(\"ml/standardized_train_indices_for_CRF_and_CLG.npy\") \n",
    "        \n",
    "        # --- Load data ---\n",
    "        df_combined = pd.read_csv(\"ml/brain_sentences_combined.csv\")\n",
    "        df_sparse = pd.read_csv(\"ml/brain_sentences_sparse.csv\")\n",
    "        assert len(df_combined) == len(df_sparse)\n",
    "        df_sparse[\"File Path\"] = df_combined[\"File Path\"]\n",
    "        df_sparse[\"report_name\"] = df_sparse[\"File Path\"].apply(lambda x: Path(x).stem)\n",
    "        #df_sparse = df_sparse.sort_values(\"report_name\").reset_index(drop=True)\n",
    "        df_sparse = df_sparse.sort_values(\n",
    "            \"report_name\",\n",
    "            kind=\"mergesort\",\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "        df_sparse.drop(columns=[\"report_name\"], inplace=True)\n",
    "        \n",
    "        OUT_OF_SCOPE_VALUE = 0.5\n",
    "      \n",
    "        \n",
    "        DO_PAIRWISE_BOW_INTERACTIONS = False \n",
    "        if DO_PAIRWISE_BOW_INTERACTIONS:\n",
    "            from itertools import combinations\n",
    "        \n",
    "            exclude_patterns = [\"Prior_\", \"_x_\", \"Facial_Bones\", \"spine_earlier\", \"Sent_Num\", \"Brain Related\"]\n",
    "            def is_bow(col):\n",
    "                return all(pat not in col for pat in exclude_patterns)\n",
    "        \n",
    "            bow_cols = [col for col in df_sparse.columns\n",
    "                        if is_bow(col)\n",
    "                        and np.issubdtype(df_sparse.dtypes[col], np.number)]\n",
    "            \n",
    "            bow_cols = [col for col in bow_cols if df_sparse[col].sum() >= 30]\n",
    "            \n",
    "            print(f\"Eligible BoW columns for pairwise interaction: {len(bow_cols)}\")\n",
    "            \n",
    "            pairwise_interactions = {}\n",
    "            for col1, col2 in combinations(bow_cols, 2):\n",
    "                inter_col = f\"BOWPAIR_{col1}_x_{col2}\"\n",
    "                pairwise_interactions[inter_col] = df_sparse[col1] * df_sparse[col2]\n",
    "            \n",
    "            print(f\"Total pairwise interactions computed: {len(pairwise_interactions)}\")\n",
    "            \n",
    "            if pairwise_interactions:\n",
    "                pairwise_df = pd.DataFrame(pairwise_interactions, index=df_sparse.index)\n",
    "            \n",
    "                interaction_freq = pairwise_df.sum(axis=0)\n",
    "            \n",
    "                top_300_inter_cols = interaction_freq.sort_values(ascending=False).head(300).index\n",
    "            \n",
    "                pairwise_df = pairwise_df[top_300_inter_cols]\n",
    "            \n",
    "                df_sparse = pd.concat([df_sparse, pairwise_df], axis=1)\n",
    "            \n",
    "                print(f\"Added {len(pairwise_df.columns)} most frequent pairwise BoW interaction columns.\") \n",
    "            else:\n",
    "                print(\"No eligible pairwise BoW interactions to add.\")\n",
    "        \n",
    "        DO_FACIAL_BONES_EARLIER = False\n",
    "        FACIAL_BONES_INTERACTION = False\n",
    "        if DO_FACIAL_BONES_EARLIER:\n",
    "            df_sentences = pd.read_csv(\"ml/brain_sentences_combined.csv\")\n",
    "            df_sparse[\"Sentence\"] = df_sentences[\"Sentence\"]\n",
    "            \n",
    "            def has_facial_bones(prior_sentences):\n",
    "                pattern = re.compile(r\"(facial bones|_ial bones)\", re.IGNORECASE)\n",
    "                return any(pattern.search(s) for s in prior_sentences)\n",
    "            \n",
    "            facial_bones_earlier = []\n",
    "            for idx in range(len(df_sparse)):\n",
    "                current_file = df_sparse.at[idx, \"File Path\"]\n",
    "                prior_indices = [i for i in range(idx) if df_sparse.at[i, \"File Path\"] == current_file]\n",
    "                prior_sents = df_sparse.loc[prior_indices, \"Sentence\"].tolist() if prior_indices else []\n",
    "                facial_bones_earlier.append(1 if has_facial_bones(prior_sents) else 0)\n",
    "            df_sparse[\"Facial_Bones_earlier\"] = facial_bones_earlier\n",
    "            df_sparse.drop(columns=[\"Sentence\"], inplace=True)\n",
    "            \n",
    "            if FACIAL_BONES_INTERACTION:\n",
    "                exclude_substrings = [\"_x_spine\", \"_x_Prior1\"]\n",
    "                eligible_cols = [\n",
    "                    col for col in df_sparse.columns\n",
    "                    if np.issubdtype(df_sparse[col].dtype, np.number)\n",
    "                    and all(substr not in col for substr in exclude_substrings)\n",
    "                    and col not in [\"Facial_Bones_earlier\", \"Brain Related\"]  # exclude label!\n",
    "                ]\n",
    "        \n",
    "                interaction_df = pd.DataFrame({\n",
    "                    f\"{col}_x_FacialBones\": df_sparse[col] * df_sparse[\"Facial_Bones_earlier\"]\n",
    "                    for col in eligible_cols\n",
    "                }, index=df_sparse.index)\n",
    "                df_sparse = pd.concat([df_sparse, interaction_df], axis=1)\n",
    "        \n",
    "        \n",
    "        y = df_sparse[\"Brain Related\"].values\n",
    "        X = df_sparse.drop(columns=[\"Brain Related\", \"File Path\"])\n",
    "\n",
    "        cols_to_zero = X.filter(regex=r'(?i)spine_earlier|sent[\\s_\\-]*num').columns\n",
    "        if len(cols_to_zero):\n",
    "            X.loc[:, cols_to_zero] = 0\n",
    "        \n",
    "        X.to_csv(\"ml/extended_base_X.parquet\", index=False)\n",
    "        np.save(\"ml/extended_base_y.npy\", y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        BASE_MODEL_C = 1.0\n",
    "        \n",
    "        \n",
    "        unique_paths = df_sparse.loc[train_indices, \"File Path\"].unique()\n",
    "        print(f\"Total train reports: {len(unique_paths)}\")\n",
    "        \n",
    "        # p already decided by loop\n",
    "        n_keep = int(p * len(unique_paths))\n",
    "        \n",
    "        if p < 1.0:\n",
    "            np.random.seed(s)\n",
    "            selected_paths = np.random.choice(unique_paths, size=n_keep, replace=False)\n",
    "            print(f\"Keeping {n_keep} reports:\", selected_paths[:5], \"...\")\n",
    "            \n",
    "            train_indices = np.array([\n",
    "                idx for idx in train_indices\n",
    "                if df_sparse.at[idx, \"File Path\"] in selected_paths\n",
    "            ])\n",
    "            \n",
    "            print(\"New train size (sentences):\", len(train_indices))\n",
    "            print(\"Unique reports now:\", \n",
    "                  np.unique(df_sparse.loc[train_indices, \"File Path\"]).shape[0])\n",
    "        \n",
    "        loo_preds = np.full(len(df_sparse), np.nan)\n",
    "        \n",
    "        LOO_K = int(400*p) # heuristic\n",
    "        if LOO_K == 1:\n",
    "            for idx in tqdm(train_indices, desc=\"LOO on train set\"):\n",
    "                train_loo = [i for i in train_indices if i != idx]\n",
    "                model = LogisticRegression(C=BASE_MODEL_C, max_iter=10000, penalty=\"l2\", solver=\"saga\", fit_intercept=False)\n",
    "                model.fit(X.iloc[train_loo], y[train_loo])\n",
    "                loo_preds[idx] = model.predict_proba(X.iloc[[idx]])[:, 1][0]\n",
    "        else:\n",
    "            train_indices_sorted = np.sort(train_indices)\n",
    "            n_train = len(train_indices_sorted)\n",
    "            for i in tqdm(range(0, n_train, LOO_K), desc=f\"Leave-{LOO_K}-out in blocks\"):\n",
    "                block = train_indices_sorted[i:i+LOO_K]\n",
    "                train_loo = [j for j in train_indices_sorted if j not in block]\n",
    "                model = LogisticRegression(C=BASE_MODEL_C, max_iter=10000, penalty=\"l2\", solver=\"saga\", fit_intercept=False)\n",
    "                model.fit(X.iloc[train_loo], y[train_loo])\n",
    "                loo_preds[block] = model.predict_proba(X.iloc[block])[:, 1]\n",
    "        \n",
    "        base_model = LogisticRegression(C=BASE_MODEL_C, max_iter=10000, penalty=\"l2\", solver=\"saga\", fit_intercept=False)\n",
    "        base_model.fit(X.iloc[train_indices], y[train_indices])\n",
    "        for idx in tqdm(test_indices, desc=\"Predicting test set\"):\n",
    "            loo_preds[idx] = base_model.predict_proba(X.iloc[[idx]])[:, 1][0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        df_sparse[\"Model_Prediction\"] = loo_preds\n",
    "        \n",
    "        df_sparse[\"Prior_1_Prediction\"] = OUT_OF_SCOPE_VALUE\n",
    "        for i in range(len(df_sparse)):\n",
    "            current_file = df_sparse.at[i, \"File Path\"]\n",
    "            if i >= 1:\n",
    "                prev_file = df_sparse.at[i - 1, \"File Path\"]\n",
    "                if current_file == prev_file:\n",
    "                    df_sparse.at[i, \"Prior_1_Prediction\"] = float(df_sparse.at[i - 1, \"Model_Prediction\"])\n",
    "                else:\n",
    "                    df_sparse.at[i, \"Prior_1_Prediction\"] = float(OUT_OF_SCOPE_VALUE)\n",
    "            else:\n",
    "                df_sparse.at[i, \"Prior_1_Prediction\"] = float(OUT_OF_SCOPE_VALUE)\n",
    "        \n",
    "        df_sparse[\"Prior_1_Prediction\"] = df_sparse[\"Prior_1_Prediction\"].astype(float)\n",
    "        df_sparse[\"Prior_1_Prediction\"] -= 0.5\n",
    "        \n",
    "        df_sparse.to_csv(\"ml/brain_sentences_sparse_meta_for_additive_logit.csv\", index=False, float_format=\"%.6f\")\n",
    "        df_sparse.drop(columns=[\"Model_Prediction\", \"File Path\"], inplace=True)\n",
    "        df_sparse.to_csv(\"ml/brain_sentences_sparse_meta.csv\", index=False, float_format=\"%.6f\")\n",
    "        print(\"Updated dataset saved with correct, leakage-free prior predictions.\")\n",
    "        \n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        df = pd.read_csv(\"ml/brain_sentences_sparse_meta_for_additive_logit.csv\")\n",
    "        \n",
    "        OUT_OF_SCOPE_VALUE = 1.0\n",
    "        \n",
    "        HOW_MUCH_HISTORY = 4 \n",
    "        for k in range(1, HOW_MUCH_HISTORY+1):\n",
    "            col_name = f\"Prior_{k}_Prediction\"\n",
    "            df[col_name] = OUT_OF_SCOPE_VALUE\n",
    "            for i in range(len(df)):\n",
    "                current_file = df.at[i, \"File Path\"]\n",
    "                if i >= k:\n",
    "                    prev_file = df.at[i - k, \"File Path\"]\n",
    "                    if current_file == prev_file:\n",
    "                        df.at[i, col_name] = float(df.at[i - k, \"Model_Prediction\"])\n",
    "                    else:\n",
    "                        df.at[i, col_name] = float(OUT_OF_SCOPE_VALUE)\n",
    "                else:\n",
    "                    df.at[i, col_name] = float(OUT_OF_SCOPE_VALUE)\n",
    "            df[col_name] = df[col_name].astype(float) - 0.5\n",
    "        \n",
    "        df[\"Prior_1234on_Avg\"] = df[[f\"Prior_{k}_Prediction\" for k in range(1, HOW_MUCH_HISTORY)]].mean(axis=1)\n",
    "        df[\"Prior_1234on_Max\"] = df[[f\"Prior_{k}_Prediction\" for k in range(1, HOW_MUCH_HISTORY)]].max(axis=1)\n",
    "        df[\"Prior_1234on_Min\"] = df[[f\"Prior_{k}_Prediction\" for k in range(1, HOW_MUCH_HISTORY)]].min(axis=1)\n",
    "        #df.drop(columns=[\"Prior_2_Prediction\", \"Prior_3_Prediction\"], inplace=True)\n",
    "        df.drop(columns=[\"Model_Prediction\"], inplace=True)\n",
    "        df = df.drop(columns=[c for c in df.columns if c.startswith(\"Prior_\")])\n",
    "        \n",
    "        from itertools import combinations\n",
    "        DO_PAIRWISE_INTERACTIONS = False\n",
    "        if DO_PAIRWISE_INTERACTIONS:\n",
    "            exclude_patterns = [\"Prior_\", \"_x_\", \"Facial_Bones\", \"spine_earlier\", \"Sent_Num\", \"Brain Related\"]\n",
    "            def is_bow(col):\n",
    "                return all(pat not in col for pat in exclude_patterns)\n",
    "            \n",
    "            bow_cols = [col for col in df.columns\n",
    "                        if is_bow(col)\n",
    "                        and np.issubdtype(df.dtypes[col], np.number)]\n",
    "            \n",
    "            bow_cols = [col for col in bow_cols if df.loc[train_indices, col].sum() >= 15]\n",
    "            \n",
    "            print(f\"Eligible BoW columns for pairwise interaction: {len(bow_cols)}\")\n",
    "            \n",
    "            pairwise_interactions = {}\n",
    "            for col1, col2 in combinations(bow_cols, 2):\n",
    "                inter_col = f\"{col1}_x_{col2}\"\n",
    "                pairwise_interactions[inter_col] = df[col1] * df[col2]\n",
    "            \n",
    "            print(f\"Total pairwise interactions computed: {len(pairwise_interactions)}\")\n",
    "            \n",
    "            if pairwise_interactions:\n",
    "                pairwise_df = pd.DataFrame(pairwise_interactions, index=df.index)\n",
    "            \n",
    "                interaction_freq = pairwise_df.loc[train_indices].sum(axis=0)\n",
    "            \n",
    "                top_300_inter_cols = interaction_freq.sort_values(ascending=False).head(1000).index\n",
    "            \n",
    "                pairwise_df = pairwise_df[top_300_inter_cols]\n",
    "            \n",
    "                df = pd.concat([df, pairwise_df], axis=1)\n",
    "            \n",
    "                print(f\"Added {len(pairwise_df.columns)} most frequent pairwise BoW interaction columns.\")\n",
    "        \n",
    "                top_300_inter_cols = interaction_freq.sort_values(ascending=False).head(300).index.tolist()\n",
    "                pd.Series(top_300_inter_cols).to_csv(\"ml/top_bow_pairwise_cols.csv\", index=False)\n",
    "            \n",
    "            else:\n",
    "                print(\"No eligible pairwise BoW interactions to add.\")\n",
    "        \n",
    "        PRIOR_INTERACTION_MODE = 1\n",
    "        exclude_cols = [col for col in df.columns if col.startswith(\"Prior_\")] + [\"Brain Related\"]\n",
    "        \n",
    "        if PRIOR_INTERACTION_MODE == 1:\n",
    "            numeric_cols = []\n",
    "        elif PRIOR_INTERACTION_MODE == 2:\n",
    "            numeric_cols = [col for col in df.columns\n",
    "                        if col not in exclude_cols\n",
    "                        and np.issubdtype(df.dtypes[col], np.number)\n",
    "                        and \"_x_spine\" not in col]\n",
    "        elif PRIOR_INTERACTION_MODE == 3:\n",
    "           numeric_cols = [col for col in df.columns\n",
    "                        if col not in exclude_cols\n",
    "                        and np.issubdtype(df.dtypes[col], np.number)]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"PRIOR_INTERACTION_MODE must be 1, 2, or 3\")\n",
    "        \n",
    "        if numeric_cols:\n",
    "            prior1 = df[\"Prior_1_Prediction\"]\n",
    "            interaction_data = {f\"{col}_x_Prior1\": df[col] * prior1 for col in numeric_cols}\n",
    "            interactions_df = pd.DataFrame(interaction_data, index=df.index)\n",
    "            df = pd.concat([df, interactions_df], axis=1)\n",
    "        \n",
    "        DO_FACIAL_BONES_EARLIER = False\n",
    "        FACIAL_BONES_INTERACTION = False\n",
    "        import re\n",
    "        if DO_FACIAL_BONES_EARLIER:\n",
    "            def has_facial_bones(prior_sentences):\n",
    "                pattern = re.compile(r\"(facial bones|_ial bones)\", re.IGNORECASE)\n",
    "                return any(pattern.search(s) for s in prior_sentences)\n",
    "            \n",
    "            facial_bones_earlier = []\n",
    "            \n",
    "            df_sentences = pd.read_csv(\"ml/brain_sentences_combined.csv\")\n",
    "            df_sentences[\"report_name\"] = df_sentences[\"File Path\"].apply(lambda fp: Path(fp).stem)\n",
    "            df_sentences.sort_values(\"report_name\", inplace=True, ignore_index=True)\n",
    "            df_sentences.drop(columns=[\"report_name\"], inplace=True)\n",
    "        \n",
    "            df[\"Sentence\"] = df_sentences[\"Sentence\"]\n",
    "            \n",
    "            for idx in range(len(df)):\n",
    "                current_file = df.at[idx, \"File Path\"]\n",
    "                prior_indices = [i for i in range(idx) if df.at[i, \"File Path\"] == current_file]\n",
    "                prior_sents = df.loc[prior_indices, \"Sentence\"].tolist() if prior_indices else []\n",
    "                facial_bones_earlier.append(1 if has_facial_bones(prior_sents) else 0)\n",
    "                \n",
    "            df[\"Facial_Bones_earlier\"] = facial_bones_earlier\n",
    "            df.drop(columns=[\"Sentence\"], inplace=True)\n",
    "        \n",
    "            if FACIAL_BONES_INTERACTION:\n",
    "                exclude_substrings = [\"_x_spine\", \"_x_Prior1\"]\n",
    "                eligible_cols = [col for col in df.columns\n",
    "                                 if np.issubdtype(df[col].dtype, np.number)\n",
    "                                 and all(substr not in col for substr in exclude_substrings)\n",
    "                                 and col not in [\"Facial_Bones_earlier\", \"Brain Related\"]]\n",
    "        \n",
    "                interaction_df = pd.DataFrame({\n",
    "                    f\"{col}_x_FacialBones\": df[col] * df[\"Facial_Bones_earlier\"]\n",
    "                    for col in eligible_cols\n",
    "                }, index=df.index)\n",
    "                df = pd.concat([df, interaction_df], axis=1)\n",
    "        \n",
    "        \n",
    "        df.to_csv(\"ml/prep_for_classical_crf.csv\", index=False, float_format=\"%.6f\")\n",
    "        \n",
    "        df.drop(columns=[\"File Path\"], inplace=True)\n",
    "        \n",
    "        df.to_csv(\"ml/brain_sentences_sparse_meta_more_aggregation.csv\", index=False, float_format=\"%.6f\")\n",
    "        print(\"Saved to ml/brain_sentences_sparse_meta_more_aggregation.csv\")\n",
    "        \n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "        from sklearn.metrics import precision_score, recall_score\n",
    "        \n",
    "        PRIOR_GENERALIZATION_SCALE_FACTOR = 1.0\n",
    "        PRIOR_SCALE_BEFORE_TO_GET_REG_APPLIED = 1.0\n",
    "        SPINE_FEATURE_SCALE = 1.0\n",
    "        PRIOR_INTERACTION_FEATURE_SCALE = 1.0\n",
    "        FACIAL_BONES_INTERACTION_SCALE = 1.0\n",
    "        CHOSEN_C = 1.0\n",
    "        \n",
    "        df = pd.read_csv(\"ml/brain_sentences_sparse_meta_more_aggregation.csv\")\n",
    "        \n",
    "        y = df[\"Brain Related\"].values\n",
    "        X = df.drop(columns=[\"Brain Related\"])\n",
    "        X.index = df.index\n",
    "\n",
    "        cols_to_zero = X.filter(regex=r'(?i)spine_earlier|sent[\\s_\\-]*num').columns\n",
    "        if len(cols_to_zero):\n",
    "            X.loc[:, cols_to_zero] = 0\n",
    "        \n",
    "        prior_cols = [col for col in X.columns if col.startswith(\"Prior_\")]\n",
    "        X[prior_cols] = X[prior_cols] * PRIOR_SCALE_BEFORE_TO_GET_REG_APPLIED\n",
    "         \n",
    "        spine_cols = [col for col in X.columns if '_x_spine' in col]\n",
    "        X[spine_cols] = X[spine_cols] * SPINE_FEATURE_SCALE\n",
    "        \n",
    "        prior1_interaction_cols = [col for col in X.columns if col.endswith('_x_Prior1')]\n",
    "        X[prior1_interaction_cols] = X[prior1_interaction_cols] * PRIOR_INTERACTION_FEATURE_SCALE\n",
    "        \n",
    "        facial_bones_interaction_cols = [col for col in X.columns if col.endswith('_x_FacialBones')]\n",
    "        X[facial_bones_interaction_cols] = X[facial_bones_interaction_cols] * FACIAL_BONES_INTERACTION_SCALE\n",
    "        \n",
    "        X_train = X.iloc[train_indices].copy()\n",
    "        X_test = X.iloc[test_indices].copy()\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        \n",
    "        for col in prior_cols:\n",
    "            if col in X_test.columns:\n",
    "                X_test[col] *= PRIOR_GENERALIZATION_SCALE_FACTOR\n",
    "        \n",
    "        model = LogisticRegression(\n",
    "            max_iter=10000, penalty=\"l2\", C=CHOSEN_C, solver=\"liblinear\", fit_intercept=False\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        meta_acc = test_accuracy\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"\\nTest Set Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "        meta_cr = classification_report(\n",
    "            y_test, y_pred, output_dict=True, zero_division=0\n",
    "        )\n",
    "\n",
    "        m_prec_0 = meta_cr['0']['precision']\n",
    "        m_rec_0  = meta_cr['0']['recall']\n",
    "        m_prec_1 = meta_cr['1']['precision']\n",
    "        m_rec_1  = meta_cr['1']['recall']\n",
    "\n",
    "        meta_f1_0 = meta_cr['0']['f1-score']\n",
    "        meta_f1_1 = meta_cr['1']['f1-score']\n",
    "        meta_f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "        meta_f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "        meta_bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "        \n",
    "                \n",
    "        import joblib\n",
    "        joblib.dump(model, \"ml/logistic_regression_brain_related_meta_more_aggregation.pkl\")\n",
    "        print(\"Meta model saved to ml/logistic_regression_brain_related_meta_more_aggregation.pkl\")\n",
    "        \n",
    "        if True:\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            cm_test = cm\n",
    "        \n",
    "            if cm_test.shape == (2, 2):\n",
    "                TN = cm_test[0, 0]\n",
    "                FP = cm_test[0, 1]\n",
    "                FN = cm_test[1, 0]\n",
    "                TP = cm_test[1, 1]\n",
    "        \n",
    "                recall_1 = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "                recall_0 = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "        \n",
    "                total = TN + FP + FN + TP\n",
    "                actual_prop_0 = (TN + FP) / total\n",
    "                actual_prop_1 = (TP + FN) / total\n",
    "                print(f\"Test set distribution: class 0 = {actual_prop_0:.2%}, class 1 = {actual_prop_1:.2%}\")\n",
    "        \n",
    "                for target_prop_0, target_prop_1 in [\n",
    "                    (0.10, 0.90),\n",
    "                    (0.15, 0.85),\n",
    "                    (0.50, 0.50),\n",
    "                    (0.90, 0.10),\n",
    "                    (actual_prop_0, actual_prop_1)\n",
    "                ]:\n",
    "                    weighted_acc = target_prop_1 * recall_1 + target_prop_0 * recall_0\n",
    "                    print(f\"Weighted accuracy ({target_prop_1:.0%} class 1, {target_prop_0:.0%} class 0): {weighted_acc:.4f}\")\n",
    "            else:\n",
    "                print(\"Unexpected confusion-matrix shape:\", cm_test.shape)\n",
    "        \n",
    "            cm = cm[::-1, ::-1]\n",
    "        \n",
    "            class_labels = [\"Brain Related\", \"Not Brain Related\"]\n",
    "        \n",
    "            plt.figure(figsize=(6, 5))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "            plt.xlabel(\"Predicted Label\")\n",
    "            plt.ylabel(\"True Label\")\n",
    "            plt.title(\"Confusion Matrix\")\n",
    "            plt.show()\n",
    "        \n",
    "            joblib.dump(model, \"ml/logistic_regression_brain_related.pkl\")\n",
    "            print(\"Model trained and saved as ml/logistic_regression_brain_related.pkl.\")\n",
    "        \n",
    "            feature_names = X.columns  \n",
    "            coefs = model.coef_[0]\n",
    "        \n",
    "            coef_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'coefficient': coefs\n",
    "            })\n",
    "        \n",
    "            coef_df['abs_coef'] = coef_df['coefficient'].abs()\n",
    "        \n",
    "            topx = coef_df.sort_values(by='abs_coef', ascending=False).head(40)\n",
    "        \n",
    "            colors = topx['coefficient'].apply(lambda x: 'blue' if x > 0 else 'red')\n",
    "        \n",
    "            plt.figure(figsize=(16, 14))\n",
    "            plt.barh(topx['feature'][::-1], topx['coefficient'][::-1], color=colors[::-1])\n",
    "            plt.xlabel(\"Coefficient Value\")\n",
    "            plt.title(\"Top Features by Coefficient Magnitude\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "            y_scores = model.decision_function(X_test)\n",
    "        \n",
    "            fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "        \n",
    "            roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "            plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "        \n",
    "            top25 = coef_df.sort_values(by='abs_coef', ascending=False).head(45)\n",
    "            \n",
    "            print(\"Top 25 features by absolute coefficient magnitude:\")\n",
    "            print(top25[['feature', 'coefficient']].to_string(index=False))\n",
    "        \n",
    "        import pandas as pd\n",
    "        from sklearn_crfsuite import CRF\n",
    "        from sklearn_crfsuite.metrics import flat_classification_report\n",
    "        from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "        \n",
    "        df = pd.read_csv(\"ml/prep_for_classical_crf.csv\")\n",
    "        \n",
    "        prior_cols = [col for col in df.columns if \"Prior\" in col]\n",
    "        df = df.drop(columns=prior_cols)\n",
    "        \n",
    "        df[\"report_name\"] = df[\"File Path\"].apply(lambda fp: Path(fp).stem)\n",
    "        df.sort_values(\n",
    "            \"report_name\",\n",
    "            kind=\"mergesort\",\n",
    "            inplace=True,\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "        zero_cols = df.filter(regex=r'(?i)spine_earlier|sent[\\s_\\-]*num').columns\n",
    "        if len(zero_cols):\n",
    "            df.loc[:, zero_cols] = 0\n",
    "        \n",
    "        df[\"Split\"] = \"unused\"\n",
    "        df.loc[train_indices, \"Split\"] = \"train\"\n",
    "        df.loc[test_indices,  \"Split\"] = \"test\"\n",
    "        \n",
    "        def row_to_feats(row):\n",
    "            return {\n",
    "                col: row[col]\n",
    "                for col in df.columns\n",
    "                if col not in (\"File Path\", \"report_name\", \"Brain Related\", \"Split\")\n",
    "            }\n",
    "        \n",
    "        X_train, y_train = [], []\n",
    "        X_test,  y_test  = [], []\n",
    "        \n",
    "        for report, group in df.groupby(\"report_name\"):\n",
    "            split = group[\"Split\"].iloc[0]\n",
    "            if split not in (\"train\", \"test\"):\n",
    "                continue\n",
    "        \n",
    "            X_seq = [row_to_feats(r) for _, r in group.iterrows()]\n",
    "            y_seq = group[\"Brain Related\"].astype(str).tolist()\n",
    "        \n",
    "            if split == \"train\":\n",
    "                X_train.append(X_seq)\n",
    "                y_train.append(y_seq)\n",
    "            else:\n",
    "                X_test.append(X_seq)\n",
    "                y_test.append(y_seq)\n",
    "        \n",
    "        df.drop(columns=[\"report_name\"], inplace=True)\n",
    "        \n",
    "        crf = CRF(\n",
    "            algorithm=\"lbfgs\",\n",
    "            c1=1.5,\n",
    "            c2=2.0,\n",
    "            max_iterations=10000,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "        crf.fit(X_train, y_train)\n",
    "        y_pred = crf.predict(X_test)\n",
    "        \n",
    "        print(flat_classification_report(\n",
    "            y_test, y_pred, labels=[\"0\", \"1\"], digits=4,\n",
    "            target_names=[\"Not Brain‑Related\", \"Brain‑Related\"]\n",
    "        ))\n",
    "        \n",
    "        y_test_flat = [item for seq in y_test for item in seq]\n",
    "        y_pred_flat = [item for seq in y_pred for item in seq]\n",
    "\n",
    "        crf_acc = accuracy_score(y_test_flat, y_pred_flat)\n",
    "        # per-class\n",
    "        c_prec_0 = precision_score(y_test_flat, y_pred_flat, pos_label='0')\n",
    "        c_rec_0  = recall_score(y_test_flat, y_pred_flat, pos_label='0')\n",
    "        c_prec_1 = precision_score(y_test_flat, y_pred_flat, pos_label='1')\n",
    "        c_rec_1  = recall_score(y_test_flat, y_pred_flat, pos_label='1')\n",
    "\n",
    "        crf_f1_0 = f1_score(y_test_flat, y_pred_flat, pos_label='0')\n",
    "        crf_f1_1 = f1_score(y_test_flat, y_pred_flat, pos_label='1')\n",
    "        crf_f1_macro = f1_score(y_test_flat, y_pred_flat, average='macro')\n",
    "        crf_f1_weighted = f1_score(y_test_flat, y_pred_flat, average='weighted')\n",
    "        crf_bal_acc = balanced_accuracy_score(y_test_flat, y_pred_flat)\n",
    "        \n",
    "\n",
    "        # append one row of results\n",
    "        results.append({\n",
    "            'p': p,\n",
    "            'seed': s,\n",
    "            # meta\n",
    "            'meta_accuracy': meta_acc,\n",
    "            'meta_balanced_accuracy': meta_bal_acc,\n",
    "            'meta_prec_0': m_prec_0,\n",
    "            'meta_rec_0':  m_rec_0,\n",
    "            'meta_f1_0':   meta_f1_0,\n",
    "            'meta_prec_1': m_prec_1,\n",
    "            'meta_rec_1':  m_rec_1,\n",
    "            'meta_f1_1':   meta_f1_1,\n",
    "            'meta_f1_macro': meta_f1_macro,\n",
    "            'meta_f1_weighted': meta_f1_weighted,\n",
    "            # crf\n",
    "            'crf_accuracy': crf_acc,\n",
    "            'crf_balanced_accuracy': crf_bal_acc,\n",
    "            'crf_prec_0':   c_prec_0,\n",
    "            'crf_rec_0':    c_rec_0,\n",
    "            'crf_f1_0':     crf_f1_0,\n",
    "            'crf_prec_1':   c_prec_1,\n",
    "            'crf_rec_1':    c_rec_1,\n",
    "            'crf_f1_1':     crf_f1_1,\n",
    "            'crf_f1_macro': crf_f1_macro,\n",
    "            'crf_f1_weighted': crf_f1_weighted,\n",
    "        })\n",
    "\n",
    "        \n",
    "        acc = accuracy_score(y_test_flat, y_pred_flat)\n",
    "        print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "        \n",
    "        bacc = balanced_accuracy_score(y_test_flat, y_pred_flat)\n",
    "        print(f\"Balanced Accuracy (macro recall): {bacc:.4f}\")\n",
    "        \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        \n",
    "        cm_test = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "        \n",
    "        if cm_test.shape == (2, 2):\n",
    "            TN = cm_test[0, 0]\n",
    "            FP = cm_test[0, 1]\n",
    "            FN = cm_test[1, 0]\n",
    "            TP = cm_test[1, 1]\n",
    "        \n",
    "            recall_1 = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "            recall_0 = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "        \n",
    "            total = TN + FP + FN + TP\n",
    "            actual_prop_0 = (TN + FP) / total\n",
    "            actual_prop_1 = (TP + FN) / total\n",
    "            print(f\"Test set distribution: class 0 = {actual_prop_0:.2%}, class 1 = {actual_prop_1:.2%}\")\n",
    "        \n",
    "            for target_prop_0, target_prop_1 in [\n",
    "                (0.10, 0.90),\n",
    "                (0.15, 0.85),\n",
    "                (0.50, 0.50),\n",
    "                (0.90, 0.10),\n",
    "                (actual_prop_0, actual_prop_1)\n",
    "            ]:\n",
    "                weighted_acc = target_prop_1 * recall_1 + target_prop_0 * recall_0\n",
    "                print(f\"Weighted accuracy ({target_prop_1:.0%} class 1, {target_prop_0:.0%} class 0): {weighted_acc:.4f}\")\n",
    "        else:\n",
    "            print(\"Unexpected confusion-matrix shape:\", cm_test.shape)\n",
    "        \n",
    "        per_report_accuracies = []\n",
    "        for y_true_seq, y_pred_seq in zip(y_test, y_pred):\n",
    "            if len(y_true_seq) == 0:\n",
    "                continue\n",
    "            n_correct = sum(int(t == p) for t, p in zip(y_true_seq, y_pred_seq))\n",
    "            acc = n_correct / len(y_true_seq)\n",
    "            per_report_accuracies.append(acc)\n",
    "        \n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.hist(per_report_accuracies, bins=15, color='dodgerblue', alpha=0.8)\n",
    "        plt.xlabel(\"Per-Report Accuracy\")\n",
    "        plt.ylabel(\"Number of Reports\")\n",
    "        plt.title(\"Histogram of CRF Accuracy per Test Report\")\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        probs_flat = []\n",
    "        labels_flat = []\n",
    "        \n",
    "        for X_seq, y_seq in zip(X_test, y_test):\n",
    "            marginals = crf.predict_marginals_single(X_seq)\n",
    "            for marg, label in zip(marginals, y_seq):\n",
    "                probs_flat.append(marg[\"1\"])\n",
    "                labels_flat.append(int(label))\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(labels_flat, probs_flat)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(7, 5))\n",
    "        plt.plot(fpr, tpr, label=f'CRF ROC (AUC = {roc_auc:.3f})', lw=2)\n",
    "        plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"CRF ROC Curve (Sentence-Level)\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "df_res = pd.DataFrame(results)\n",
    "df_res.to_csv(\"ml/meta_crf_metrics_diff_dataset_size_seed.csv\", index=False)\n",
    "print(\"Saved ml/meta_crf_metrics_diff_dataset_size_seed.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Zakk Lab 617",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555828c-3566-423d-ade7-3687d6b15fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def split_reports(input_csv: str, output_dir: str):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    df[\"report_name\"] = df[\"File Path\"].apply(lambda x: Path(x).stem)\n",
    "    base = Path(output_dir)\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    for report_name, group in df.groupby(\"report_name\"):\n",
    "        report_dir = base / report_name\n",
    "        report_dir.mkdir(parents=True, exist_ok=True)\n",
    "        group.drop(columns=[\"report_name\"]).to_csv(report_dir / \"report.csv\", index=False)\n",
    "if __name__ == \"__main__\":\n",
    "    split_reports(\n",
    "        input_csv=\"ml/brain_sentences_combined.csv\", # don't use full, because this is the filtered version\n",
    "        output_dir=\"ml/CRF_individual\"\n",
    "    )\n",
    "    print(\"Finished splitting into individual report CSVs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc402c-fc48-4d86-84df-88f6dd2b9bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas scikit-learn matplotlib\n",
    "#!pip install transformers==4.37.0\n",
    "#!pip install torch==2.1.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "import sys\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "#!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torchcrf import CRF\n",
    "#!pip install pytorch-crf\n",
    "#from TorchCRF import CRF\n",
    "from torchcrf import CRF\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# torchcrf doesn't natively support marginal probabilities so we implement our own non-brute force method to get them using the markov assumption\n",
    "def compute_log_alpha(crf, emissions, mask):\n",
    "    seq_length, batch_size, num_tags = emissions.shape\n",
    "\n",
    "    score = crf.start_transitions + emissions[0] # (batch_size, num_tags)\n",
    "    history = [score]\n",
    "    for i in range(1, seq_length):\n",
    "        broadcast_score = score.unsqueeze(2) # (batch_size, num_tags, 1)\n",
    "        broadcast_emission = emissions[i].unsqueeze(1) # (batch_size, 1, num_tags)\n",
    "        # Transition + emission scores\n",
    "        next_score = broadcast_score + crf.transitions + broadcast_emission # (batch_size, num_tags, num_tags)\n",
    "        next_score = torch.logsumexp(next_score, dim=1) # (batch_size, num_tags)\n",
    "        score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "        history.append(score)\n",
    "    return torch.stack(history) # (seq_length, batch_size, num_tags)\n",
    "\n",
    "\n",
    "def compute_log_beta(crf, emissions, mask):\n",
    "    seq_len, batch_size, num_tags = emissions.shape\n",
    "\n",
    "    score = crf.end_transitions.unsqueeze(0).expand(batch_size, num_tags) # (batch_size, num_tags)\n",
    "    history = [score]\n",
    "\n",
    "    for i in reversed(range(seq_len - 1)):\n",
    "        bs = score.unsqueeze(1) # (batch_size, 1, num_tags)\n",
    "        be = emissions[i+1].unsqueeze(1) # (batch_size, 1, num_tags)\n",
    "        # (batch_size, num_tags, num_tags)\n",
    "        all_paths = bs + be + crf.transitions\n",
    "        next_score = torch.logsumexp(all_paths, dim=2) # (batch_size, num_tags)\n",
    "        mask_i1 = mask[i+1].unsqueeze(1) # (batch_size, 1)\n",
    "        score = torch.where(mask_i1, next_score, score)\n",
    "        history.insert(0, score)\n",
    "\n",
    "    return torch.stack(history) # (seq_len, batch_size, num_tags)\n",
    "\n",
    "def crf_marginals(crf, emissions, mask):\n",
    "    seq_len, batch_size, num_tags = emissions.shape\n",
    "    log_alpha = compute_log_alpha(crf, emissions, mask)\n",
    "    log_beta = compute_log_beta(crf, emissions, mask)\n",
    "    \n",
    "    lengths = mask.sum(dim=0)\n",
    "    last_indices = (lengths - 1).clamp(min=0)\n",
    "    batch_indices = torch.arange(batch_size)\n",
    "    last_log_alpha = log_alpha[last_indices, batch_indices, :]\n",
    "    log_Z = torch.logsumexp(last_log_alpha + crf.end_transitions, dim=1, keepdim=True)\n",
    "    \n",
    "    log_marg = log_alpha + log_beta - log_Z.unsqueeze(0)\n",
    "    \n",
    "    mask_t = mask.unsqueeze(2)\n",
    "    log_marg = torch.where(mask_t, log_marg, torch.tensor(float('-inf'), device=log_marg.device))\n",
    "    marginals = log_marg.exp()\n",
    "    \n",
    "    sums = marginals.sum(dim=2, keepdim=True)\n",
    "    sums = torch.where(sums > 0, sums, torch.ones_like(sums))\n",
    "    marginals = marginals / sums\n",
    "    \n",
    "    return marginals\n",
    "\n",
    "USE_BIG_BERT = True\n",
    "if USE_BIG_BERT:\n",
    "    MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    #MODEL_NAME = \"bert-base-uncased\"\n",
    "    BERT_SIZE = 768\n",
    "else:\n",
    "    MODEL_NAME = \"prajjwal1/bert-tiny\"  # lower‐mem model\n",
    "    BERT_SIZE = 128\n",
    "\n",
    "if hasattr(torch, \"get_default_device\"):\n",
    "    DEVICE = torch.get_default_device()\n",
    "else:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "SEED         = 42\n",
    "NUM_LABELS   = 2\n",
    "MAX_SENT_LEN = 128\n",
    "DATA_ROOT    = \"ml/CRF_individual\"\n",
    "CACHE_BASE   = \"ml/CRF_cache\"\n",
    "EPOCHS       = 305\n",
    "LR = 5e-4\n",
    "DROPOUTP = 0.6\n",
    "L2_REG = 0.05\n",
    "USE_EXTRA_NONLINEARITIES = True\n",
    "\n",
    "PROJ_DIM     = 768 # choose 768 (or BERT embedding) to not learn a projection layer\n",
    "USE_ATTENTION = True\n",
    "ATTN_DIM = 256 # 1 head\n",
    "CONCAT_NOW_WITH_ATTENTION = True  # recommended\n",
    "USE_VALUE_UP_DOWN_TO_SAVE_ATT_PARAMETERS = False  # if doing attention, do a low rank transformation to reduce the number of parameters\n",
    "VALUE_BOTTLENECK_DIM = 64 # only if boolean is True does this matter\n",
    "USE_MLP_INTERACTION = True\n",
    "MLP_OUTPUT_DIM = 128 # doesn't matter if MLP is false\n",
    "\n",
    "#random.seed(SEED)\n",
    "#torch.manual_seed(SEED)\n",
    "#torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "import joblib\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "USE_BOW_FOR_NOW = False  # Set to True for BoW, False for BERT for \"now\" sentence\n",
    "BOW_VECTOR_PATH = \"ml/no_interactions_for_crf_bow_local_vectorizer.pkl\"\n",
    "BOW_EMBEDDING_SIZE = 768  # pad/truncate BoW to match BERT_SIZE\n",
    "\n",
    "bow_vectorizer = joblib.load(BOW_VECTOR_PATH)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# option to use some bow embeddings here but not used in final version\n",
    "def get_bow_embedding(text):\n",
    "    text_clean = clean_text(text)\n",
    "    bow_vec = bow_vectorizer.transform([text_clean]).toarray()[0]\n",
    "    arr = np.zeros(BOW_EMBEDDING_SIZE, dtype=np.float32)\n",
    "    length = min(len(bow_vec), BOW_EMBEDDING_SIZE)\n",
    "    arr[:length] = bow_vec[:length]\n",
    "    return torch.tensor(arr)\n",
    "\n",
    "\n",
    "# limiting to a fixed amount of context for consistent passage embeddings\n",
    "USE_DYNAMIC_CONTEXT = False   # True = dynamic context, False = full context\n",
    "def get_dynamic_context(sents, idx, min_words=4, direction=\"before\"):\n",
    "    context_sents = []\n",
    "    count = 0\n",
    "\n",
    "    if direction == \"before\":\n",
    "        # walk backwards from idx-1 down to 0\n",
    "        for j in range(idx-1, -1, -1):\n",
    "            context_sents.insert(0, sents[j])  # keep original order\n",
    "            count = sum(len(s.split()) for s in context_sents)\n",
    "            if count >= min_words:\n",
    "                break\n",
    "\n",
    "    elif direction == \"after\":\n",
    "        # walk forwards from idx+1 up to len(sents)-1\n",
    "        for j in range(idx+1, len(sents)):\n",
    "            context_sents.append(sents[j])\n",
    "            count = sum(len(s.split()) for s in context_sents)\n",
    "            if count >= min_words:\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"direction must be 'before' or 'after'\")\n",
    "\n",
    "    return \" \".join(context_sents)\n",
    "\n",
    "def model_name_to_folder(model_name):\n",
    "    return model_name.replace(\"/\", \"_\")\n",
    "\n",
    "AORB            = False\n",
    "POOLING_STRATEGY = \"cls\" # \"cls\", \"mean\", or \"max\" embeddings\n",
    "\n",
    "MODEL_FOLDER = model_name_to_folder(MODEL_NAME)\n",
    "CACHE_DIR = os.path.join(CACHE_BASE, POOLING_STRATEGY, MODEL_FOLDER)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def get_pooled_embedding(model, tokenizer, text, strategy):\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_SENT_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "        hidden = out.last_hidden_state\n",
    "        mask   = enc[\"attention_mask\"].unsqueeze(-1)\n",
    "    if strategy == \"cls\":\n",
    "        return hidden[:, 0, :].squeeze(0).cpu()\n",
    "    elif strategy == \"mean\":\n",
    "        m_hidden = hidden * mask\n",
    "        summed   = m_hidden.sum(dim=1)\n",
    "        counts   = mask.sum(dim=1)\n",
    "        return (summed / counts).squeeze(0).cpu()\n",
    "    elif strategy == \"max\":\n",
    "        mask_exp = mask.expand_as(hidden)\n",
    "        hidden[mask_exp == 0] = -1e9\n",
    "        return hidden.max(dim=1).values.squeeze(0).cpu()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pooling strategy: {strategy}\")\n",
    "\n",
    "def preprocess_embeddings(report_dirs, tokenizer, bert):\n",
    "    for d in report_dirs:\n",
    "        name       = os.path.basename(d)\n",
    "        cache_path = os.path.join(CACHE_DIR, f\"{name}.pt\")\n",
    "        if os.path.exists(cache_path):\n",
    "            continue\n",
    "\n",
    "        df        = pd.read_csv(os.path.join(d, \"report.csv\"))\n",
    "        sents     = df[\"Sentence\"].tolist()\n",
    "        labels    = torch.tensor(df[\"Brain Related\"].tolist(), dtype=torch.long)\n",
    "        befores, nows, afters = [], [], []\n",
    "\n",
    "        N = len(sents)\n",
    "        for i in range(N):\n",
    "            if USE_DYNAMIC_CONTEXT:\n",
    "                tb = get_dynamic_context(sents, i, min_words=4, direction=\"before\")\n",
    "                ta = get_dynamic_context(sents, i, min_words=4, direction=\"after\")\n",
    "            else:\n",
    "                tb = \" \".join(sents[:i])     if i > 0   else \"\"\n",
    "                ta = \" \".join(sents[i+1:])   if i < N-1 else \"\"\n",
    "            tn = sents[i]\n",
    "\n",
    "            befores.append(get_pooled_embedding(bert, tokenizer, tb, POOLING_STRATEGY))\n",
    "            if USE_BOW_FOR_NOW:\n",
    "                nows.append(get_bow_embedding(tn))\n",
    "            else:\n",
    "                nows.append(get_pooled_embedding(bert, tokenizer, tn, POOLING_STRATEGY))\n",
    "            afters .append(get_pooled_embedding(bert, tokenizer, ta, POOLING_STRATEGY))\n",
    "\n",
    "        torch.save({\n",
    "            \"before\": torch.stack(befores),\n",
    "            \"now\":    torch.stack(nows),\n",
    "            \"after\":  torch.stack(afters),\n",
    "            \"labels\": labels\n",
    "        }, cache_path)\n",
    "        print(f\"Cached {name} → {cache_path}\")\n",
    "\n",
    "class CachedReportDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(self.files[idx])\n",
    "        return (\n",
    "            data[\"before\"].to(DEVICE),\n",
    "            data[\"now\"   ].to(DEVICE),\n",
    "            data[\"after\" ].to(DEVICE),\n",
    "            data[\"labels\"].to(DEVICE)\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # assuming batch_size=1\n",
    "    return batch[0]\n",
    "\n",
    "class NeuralCRF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.use_attention = USE_ATTENTION\n",
    "        self.use_mlp_interaction = USE_MLP_INTERACTION\n",
    "        self.concat_now_with_attention = CONCAT_NOW_WITH_ATTENTION\n",
    "        self.dropout = nn.Dropout(DROPOUTP)\n",
    "        \n",
    "        if PROJ_DIM == BERT_SIZE:\n",
    "            self.proj = nn.Identity()\n",
    "        else:\n",
    "            self.proj = nn.Linear(BERT_SIZE, PROJ_DIM)\n",
    "            \n",
    "        if self.use_attention:\n",
    "            self.attn_dim = ATTN_DIM\n",
    "            # for each context vector: project to attn_dim for query/key/value\n",
    "            self.attn_q = nn.Linear(PROJ_DIM, ATTN_DIM)\n",
    "            self.attn_k = nn.Linear(PROJ_DIM, ATTN_DIM)\n",
    "\n",
    "            if not USE_VALUE_UP_DOWN_TO_SAVE_ATT_PARAMETERS:\n",
    "                self.attn_v = nn.Linear(PROJ_DIM, ATTN_DIM)\n",
    "            else:\n",
    "                self.attn_v_down = nn.Linear(PROJ_DIM, VALUE_BOTTLENECK_DIM)\n",
    "                self.attn_v_up   = nn.Linear(VALUE_BOTTLENECK_DIM, ATTN_DIM)\n",
    "\n",
    "        else:\n",
    "            self.attn_dim = None\n",
    "            \n",
    "        if self.use_attention and self.concat_now_with_attention:\n",
    "            input_dim = self.attn_dim + PROJ_DIM\n",
    "        elif self.use_attention:\n",
    "            input_dim = self.attn_dim\n",
    "        else:\n",
    "            input_dim = 3 * PROJ_DIM\n",
    "\n",
    "        if self.use_mlp_interaction:\n",
    "            self.interaction = nn.Sequential(\n",
    "                nn.Linear(input_dim, MLP_OUTPUT_DIM),\n",
    "                nn.ELU(alpha=1.0), # nonlinearity\n",
    "                self.dropout\n",
    "            )\n",
    "            self.fc = nn.Linear(MLP_OUTPUT_DIM, NUM_LABELS)\n",
    "        else:\n",
    "            self.interaction = nn.Identity()\n",
    "            self.fc = nn.Linear(input_dim, NUM_LABELS)\n",
    "      \n",
    "        self.crf  = CRF(NUM_LABELS)\n",
    "\n",
    "    def forward(self, x_b, x_n, x_a, labels=None):\n",
    "        if USE_EXTRA_NONLINEARITIES:\n",
    "            extra_act_fn = nn.ELU(alpha=1.0)\n",
    "        else:\n",
    "            extra_act_fn = nn.Identity()\n",
    "            \n",
    "        pb = self.dropout(extra_act_fn(self.proj(x_b)))\n",
    "        pn = self.dropout(extra_act_fn(self.proj(x_n)))\n",
    "        pa = self.dropout(extra_act_fn(self.proj(x_a)))\n",
    "\n",
    "\n",
    "        if self.use_attention: \n",
    "            # stack contexts: (S, 3, D)\n",
    "            contexts = torch.stack([pb, pn, pa], dim=1)\n",
    "            # project all to K, V: (S, 3, ATTN_DIM)\n",
    "            keys   = self.attn_k(contexts)\n",
    "            if not USE_VALUE_UP_DOWN_TO_SAVE_ATT_PARAMETERS:\n",
    "                values = self.attn_v(contexts) # (S, 3, ATTN_DIM)\n",
    "            else:\n",
    "                v_down = extra_act_fn(self.attn_v_down(contexts)) # (S, 3, BOTTLENECK)\n",
    "                values = self.attn_v_up(v_down)    \n",
    "\n",
    "            # project current sentence only to query: (S, ATTN_DIM)\n",
    "            query = self.attn_q(pn).unsqueeze(1)  # shape: (S, 1, ATTN_DIM)\n",
    "\n",
    "            # compute attention scores: (S, 1, 3)\n",
    "            attn_scores = torch.matmul(query, keys.transpose(1, 2)) / (ATTN_DIM ** 0.5)\n",
    "            attn_weights = torch.softmax(attn_scores, dim=-1) # shape: (S, 1, 3)\n",
    "\n",
    "            # weighted sum over values: (S, 1, ATTN_DIM)\n",
    "            attn_vec = torch.matmul(attn_weights, values).squeeze(1) # (S, ATTN_DIM)\n",
    "\n",
    "            attn_vec = self.dropout(extra_act_fn(attn_vec))\n",
    "            \n",
    "            if self.concat_now_with_attention:\n",
    "                h = torch.cat([attn_vec, pn], dim=1) # (S, ATTN_DIM + PROJ_DIM)\n",
    "            else:\n",
    "                h = attn_vec # (S, ATTN_DIM)\n",
    "        else:\n",
    "            h = torch.cat([pb, pn, pa], dim=1)  # (S, 3*proj_dim)\n",
    "\n",
    "        h = self.interaction(h) # nonlinearity built in if doing anything\n",
    "        emis = self.fc(h).unsqueeze(1)         # (S, 1, num_labels)\n",
    "        mask = torch.ones(emis.shape[:2], dtype=torch.bool, device=emis.device)\n",
    "\n",
    "        if labels is not None: # loss is 1 scalar number per report\n",
    "            lbl = labels.unsqueeze(1)\n",
    "            return -self.crf(emis, lbl, mask=mask)\n",
    "        else: # inference\n",
    "            return self.crf.decode(emis, mask=mask)[0]\n",
    "\n",
    "    def get_emissions(self, b, n, a):\n",
    "        # use same code as in forward, but return the emissions\n",
    "        if USE_EXTRA_NONLINEARITIES:\n",
    "            extra_act_fn = nn.ELU(alpha=1.0)\n",
    "        else:\n",
    "            extra_act_fn = nn.Identity()\n",
    "        pb = self.dropout(extra_act_fn(self.proj(b)))\n",
    "        pn = self.dropout(extra_act_fn(self.proj(n)))\n",
    "        pa = self.dropout(extra_act_fn(self.proj(a)))\n",
    "        if self.use_attention:\n",
    "            contexts = torch.stack([pb, pn, pa], dim=1)\n",
    "            keys   = self.attn_k(contexts)\n",
    "            if not USE_VALUE_UP_DOWN_TO_SAVE_ATT_PARAMETERS:\n",
    "                values = self.attn_v(contexts)\n",
    "            else:\n",
    "                v_down = extra_act_fn(self.attn_v_down(contexts))\n",
    "                values = self.attn_v_up(v_down)\n",
    "            query = self.attn_q(pn).unsqueeze(1)\n",
    "            attn_scores = torch.matmul(query, keys.transpose(1, 2)) / (ATTN_DIM ** 0.5)\n",
    "            attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "            attn_vec = torch.matmul(attn_weights, values).squeeze(1)\n",
    "            attn_vec = self.dropout(extra_act_fn(attn_vec))\n",
    "            if self.concat_now_with_attention:\n",
    "                h = torch.cat([attn_vec, pn], dim=1)\n",
    "            else:\n",
    "                h = attn_vec\n",
    "        else:\n",
    "            h = torch.cat([pb, pn, pa], dim=1)\n",
    "        h = self.interaction(h)\n",
    "        emissions = self.fc(h).unsqueeze(1)\n",
    "        return emissions  # (seq_len, 1, num_labels)\n",
    "\n",
    "\n",
    "def train(p=1.0):\n",
    "    with open(\"ml/from_classical_standardized_train_report_names.json\") as f:\n",
    "        train_report_names = json.load(f)\n",
    "    with open(\"ml/from_classical_standardized_test_report_names.json\") as f:\n",
    "        test_report_names = json.load(f)\n",
    "\n",
    "    all_report_dirs = sorted(\n",
    "        os.path.join(DATA_ROOT, d)\n",
    "        for d in os.listdir(DATA_ROOT)\n",
    "        if os.path.isdir(os.path.join(DATA_ROOT, d))\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    train_dirs_all = [d for d in all_report_dirs if os.path.basename(d) in train_report_names]\n",
    "    test_dirs      = [d for d in all_report_dirs if os.path.basename(d) in test_report_names]\n",
    "\n",
    "    # can evaluate how good the crf does with less data\n",
    "    np.random.seed(SEED)\n",
    "    sample_size = max(1, int(p * len(train_dirs_all)))\n",
    "    train_dirs_all = list(np.random.choice(train_dirs_all, size=sample_size, replace=False))\n",
    "\n",
    "\n",
    "    print(\"Number of train_dirs:\", len(train_dirs_all))\n",
    "    print(\"Example train_dirs:\", train_dirs_all[:5])\n",
    "    missing = [d for d in train_dirs_all if not os.path.exists(d)]\n",
    "    print(\"Missing train report directories:\", missing)\n",
    "\n",
    "    with open(\"ml/from_classical_standardized_train_report_names.json\") as f:\n",
    "        train_report_names = json.load(f)\n",
    "    print(train_report_names[:5])\n",
    "\n",
    "    print([f for f in os.listdir(\"ml/CRF_individual\")][:5])\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    perm = np.random.permutation(len(train_dirs_all))\n",
    "    #val_size = max(1, int((0.05 + (0.25 - p / 4)) * len(train_dirs_all)))\n",
    "    val_size = max(1, int(((1 - p) * 5 + p * 9)))\n",
    "    val_dirs  = [train_dirs_all[i] for i in perm[:val_size]]\n",
    "    train_dirs = [train_dirs_all[i] for i in perm[val_size:]]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    bert      = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
    "    preprocess_embeddings(train_dirs + val_dirs + test_dirs, tokenizer, bert)\n",
    "\n",
    "    def get_filelist_from_dirs(dirs):\n",
    "        return [os.path.join(CACHE_DIR, f\"{os.path.basename(d)}.pt\") for d in dirs]\n",
    "\n",
    "    train_files = get_filelist_from_dirs(train_dirs)\n",
    "    val_files   = get_filelist_from_dirs(val_dirs)\n",
    "    test_files  = get_filelist_from_dirs(test_dirs)\n",
    "\n",
    "    print(f\"Num train files: {len(train_files)}\")\n",
    "    print(f\"Num val files:   {len(val_files)}\")\n",
    "    print(f\"Num test files:  {len(test_files)}\")\n",
    "\n",
    "    ds_train = CachedReportDataset(train_files)\n",
    "    ds_val   = CachedReportDataset(val_files)\n",
    "    ds_test  = CachedReportDataset(test_files)\n",
    "\n",
    "    dl_train = DataLoader(ds_train, batch_size=1, shuffle=True,  collate_fn=collate_fn)\n",
    "    dl_val   = DataLoader(ds_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "    dl_test  = DataLoader(ds_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    hid   = bert.config.hidden_size\n",
    "    model = NeuralCRF().to(DEVICE)\n",
    "    opt   = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay = L2_REG)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "    patience = 12 # if validation loss is more than the best validation loss for x epochs or more, stop.\n",
    "    bad_epochs = 0\n",
    "    ckpt_path = \"ml/best_model_weights.pt\"\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        loss_acc = 0.0\n",
    "        for b, n, a, y in dl_train:\n",
    "            opt.zero_grad()\n",
    "            l = model(b, n, a, y)\n",
    "            l.backward()\n",
    "            opt.step()\n",
    "            loss_acc += l.item()\n",
    "        avg_train_loss = loss_acc/len(dl_train)\n",
    "        model.eval()\n",
    "        val_acc = 0.0\n",
    "        with torch.no_grad():\n",
    "            for b, n, a, y in dl_val:\n",
    "                val_acc += model(b, n, a, y).item()\n",
    "        avg_val_loss = val_acc/len(dl_val)\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} train_loss={avg_train_loss:.4f}  val_loss={avg_val_loss:.4f}\")\n",
    "        # early stopping + save best\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_state, ckpt_path)  # backup to disk\n",
    "            best_epoch = epoch\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best validation loss: {best_val_loss:.4f} (epoch {best_epoch})\")\n",
    "                break\n",
    "    # restore best model weights\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
    "\n",
    "    # evaluations\n",
    "    model.eval()\n",
    "\n",
    "    train_preds, train_labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for b, n, a, y in dl_train:\n",
    "            p = model(b, n, a)\n",
    "            train_preds.extend(p)\n",
    "            train_labs.extend(y.tolist())\n",
    "    print(\"Train label distribution:\", Counter(train_labs))\n",
    "    print(f\"\\nTrain Accuracy: {accuracy_score(train_labs, train_preds):.4f}\")\n",
    "    cm_train = confusion_matrix(train_labs, train_preds)\n",
    "    disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train)\n",
    "    disp_train.plot(cmap=\"Blues\")\n",
    "    plt.title(\"Train Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for b, n, a, y in dl_test:\n",
    "            p = model(b, n, a)\n",
    "            preds.extend(p)\n",
    "            labs .extend(y.tolist())\n",
    "    print(\"Test label distribution:\", Counter(labs))\n",
    "    print(f\"\\nTest Accuracy: {accuracy_score(labs, preds):.4f}\")\n",
    "    print(classification_report(labs, preds, digits=4))\n",
    "    cm_test = confusion_matrix(labs, preds)\n",
    "    disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test)\n",
    "    disp_test.plot(cmap=\"Blues\")\n",
    "    plt.title(\"Test Confusion Matrix\")\n",
    "    plt.show()\n",
    "    if cm_test.shape == (2, 2):\n",
    "        TN = cm_test[0, 0]\n",
    "        FP = cm_test[0, 1]\n",
    "        FN = cm_test[1, 0]\n",
    "        TP = cm_test[1, 1]\n",
    "\n",
    "        recall_1 = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        recall_0 = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "\n",
    "        total = TN + FP + FN + TP\n",
    "        actual_prop_0 = (TN + FP) / total\n",
    "        actual_prop_1 = (TP + FN) / total\n",
    "        print(f\"Test set distribution: class 0 = {actual_prop_0:.2%}, class 1 = {actual_prop_1:.2%}\")\n",
    "\n",
    "        for target_prop_0, target_prop_1 in [(0.10, 0.90), (0.15, 0.85), (0.50, 0.50), (actual_prop_0, actual_prop_1)]:\n",
    "            weighted_acc = target_prop_1 * recall_1 + target_prop_0 * recall_0\n",
    "            print(f\"Weighted accuracy ({target_prop_1:.0%} class 1, {target_prop_0:.0%} class 0): {weighted_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nWeighted accuracy calculation skipped: non-binary confusion matrix shape:\", cm_test.shape)\n",
    "\n",
    "    # evaluate on special report\n",
    "    ABDOMEN_REPORT = \"abdomenetc.csv\"\n",
    "    if os.path.exists(ABDOMEN_REPORT):\n",
    "        df_abd = pd.read_csv(ABDOMEN_REPORT)\n",
    "        sents_abd = df_abd[\"Sentence\"].tolist()\n",
    "\n",
    "        befores, nows, afters = [], [], []\n",
    "        N = len(sents_abd)\n",
    "        for i in range(N):\n",
    "            tb = \" \".join(sents_abd[:i])     if i > 0   else \"\"\n",
    "            tn = sents_abd[i]\n",
    "            ta = \" \".join(sents_abd[i+1:])   if i < N-1 else \"\"\n",
    "            befores.append(get_pooled_embedding(bert, tokenizer, tb, POOLING_STRATEGY))\n",
    "            nows   .append(get_pooled_embedding(bert, tokenizer, tn, POOLING_STRATEGY))\n",
    "            afters .append(get_pooled_embedding(bert, tokenizer, ta, POOLING_STRATEGY))\n",
    "\n",
    "        befores = torch.stack(befores).to(DEVICE)\n",
    "        nows    = torch.stack(nows).to(DEVICE)\n",
    "        afters  = torch.stack(afters).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_abd = model(befores, nows, afters)\n",
    "\n",
    "        if \"Brain Related\" in df_abd.columns:\n",
    "            y_true_abd = df_abd[\"Brain Related\"].values.astype(int)\n",
    "            mask = y_true_abd != -1  # skip unknown labels\n",
    "            acc_abd = accuracy_score(y_true_abd[mask], np.array(y_pred_abd)[mask])\n",
    "            print(f\"\\nAccuracy on abdomenetc.csv: {acc_abd:.4f}\")\n",
    "        else:\n",
    "            acc_abd = None\n",
    "            print(\"\\nNo 'Brain Related' column in abdomenetc.csv for accuracy scoring.\")\n",
    "    else:\n",
    "        acc_abd = None\n",
    "        print(\"\\nadbomenetc.csv not found for evaluation.\")\n",
    "\n",
    "    if True:    \n",
    "        THRESH_STEP = 0.001  # very fine grid\n",
    "        OUT_THRESHOLD_CSV = \"ml/neuralcrf_threshold_metrics.csv\"\n",
    "        OUT_THRESHOLD_NPZ = \"ml/neuralcrf_test_marginals.npz\"\n",
    "    \n",
    "        all_probs, all_labels = [], []\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for b, n, a, y in dl_test:\n",
    "                emissions = model.get_emissions(b, n, a)\n",
    "    \n",
    "                mask = torch.ones(emissions.shape[:2], dtype=torch.bool, device=emissions.device)\n",
    "    \n",
    "                marg = crf_marginals(model.crf, emissions, mask)\n",
    "    \n",
    "                m = marg.squeeze(1)\n",
    "                y_prob = m[:, 1].detach().cpu().numpy()\n",
    "    \n",
    "                y_true = y.detach().cpu().numpy()\n",
    "    \n",
    "                all_probs.append(y_prob)\n",
    "                all_labels.append(y_true)\n",
    "    \n",
    "        probs_flat  = np.concatenate(all_probs).astype(np.float64)\n",
    "        labels_flat = np.concatenate(all_labels).astype(np.int64)\n",
    "    \n",
    "        valid = np.isin(labels_flat, [0, 1])\n",
    "        probs_flat  = probs_flat[valid]\n",
    "        labels_flat = labels_flat[valid]\n",
    "\n",
    "        roc_auc = roc_auc_score(labels_flat, probs_flat)\n",
    "        print(f\"ROC AUC (Neural CRF marginals on test): {roc_auc:.4f}\")\n",
    "    \n",
    "        thresholds = np.arange(0.0, 1.0 + THRESH_STEP, THRESH_STEP)\n",
    "        rows = []\n",
    "        for t in thresholds:\n",
    "            preds = (probs_flat >= t).astype(int)\n",
    "            acc = accuracy_score(labels_flat, preds)\n",
    "            f1_cls1 = f1_score(labels_flat, preds, pos_label=1, zero_division=0)\n",
    "            f1_macro = f1_score(labels_flat, preds, average=\"macro\", zero_division=0)\n",
    "            rows.append((t, f1_macro, f1_cls1, acc))\n",
    "    \n",
    "        th_df = pd.DataFrame(rows, columns=[\"threshold\", \"MacroF1\", \"F1_class1\", \"accuracy\"])\n",
    "        os.makedirs(os.path.dirname(OUT_THRESHOLD_CSV), exist_ok=True)\n",
    "        th_df.to_csv(OUT_THRESHOLD_CSV, index=False)\n",
    "        np.savez_compressed(OUT_THRESHOLD_NPZ, probs=probs_flat, labels=labels_flat)\n",
    "    \n",
    "        print(f\"Saved CRF marginal threshold sweep to {OUT_THRESHOLD_CSV}\")\n",
    "        print(f\"Saved raw probs/labels to {OUT_THRESHOLD_NPZ}\")\n",
    "\n",
    "    \n",
    "    if True:\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "        #SYNTHETIC_DIR = \"MIMIC/real_individual_reports\"\n",
    "        SYNTHETIC_DIR = \"MIMIC/synthetic_bundled_reports_deterministic\"\n",
    "        synthetic_csvs = [os.path.join(SYNTHETIC_DIR, f) for f in os.listdir(SYNTHETIC_DIR) if f.endswith('.csv')]\n",
    "        \n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "    \n",
    "        for csv_path in synthetic_csvs:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            sents = df[\"Sentence\"].tolist()\n",
    "            # for each sentence, build befores/nows/afters as before\n",
    "            befores, nows, afters = [], [], []\n",
    "            N = len(sents)\n",
    "            for i in range(N):\n",
    "                tb = \" \".join(sents[:i])     if i > 0   else \"\"\n",
    "                tn = sents[i]\n",
    "                ta = \" \".join(sents[i+1:])   if i < N-1 else \"\"\n",
    "                befores.append(get_pooled_embedding(bert, tokenizer, tb, POOLING_STRATEGY))\n",
    "                nows   .append(get_pooled_embedding(bert, tokenizer, tn, POOLING_STRATEGY))\n",
    "                afters .append(get_pooled_embedding(bert, tokenizer, ta, POOLING_STRATEGY))\n",
    "            befores = torch.stack(befores).to(DEVICE)\n",
    "            nows    = torch.stack(nows).to(DEVICE)\n",
    "            afters  = torch.stack(afters).to(DEVICE)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                emissions = model.get_emissions(befores, nows, afters)\n",
    "                mask = torch.ones((N, 1), dtype=torch.bool, device=DEVICE)\n",
    "                marginals = crf_marginals(model.crf, emissions, mask)\n",
    "                m = marginals.squeeze(1)\n",
    "                y_prob = m[:, 1].cpu().numpy()\n",
    "                # also get hard predictions from the marginals\n",
    "                y_pred = y_prob >= 0.5\n",
    "    \n",
    "            if \"Brain Related\" in df.columns:\n",
    "                y_true = df[\"Brain Related\"].values.astype(int)\n",
    "                mask_ = y_true != -1\n",
    "                if mask_.sum() > 0:\n",
    "                    acc = accuracy_score(y_true[mask_], y_pred[mask_])\n",
    "                    print(f\"File: {os.path.basename(csv_path):45s} | Accuracy: {acc:.4f}\")\n",
    "                    all_labels.append(y_true[mask_])\n",
    "                    all_preds.append(y_pred[mask_])\n",
    "                    all_probs.append(y_prob[mask_])\n",
    "            else:\n",
    "                print(f\"File: {os.path.basename(csv_path):45s} | No 'Brain Related' labels found.\")\n",
    "    \n",
    "        if all_labels:\n",
    "            all_labels = np.concatenate(all_labels)\n",
    "            all_probs = np.concatenate(all_probs)\n",
    "            all_preds = np.concatenate(all_preds)\n",
    "\n",
    "            np.save(\"neuralcrf_mimic_probabilities.npy\", all_probs)\n",
    "    \n",
    "            accuracy = accuracy_score(all_labels, all_preds)\n",
    "            print(f\"Overall accuracy: {accuracy:.4f}\")\n",
    "            if np.unique(all_labels).size > 1:\n",
    "                fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.figure(figsize=(7,7))\n",
    "                plt.plot(fpr, tpr, label=f'Neural CRF (AUC = {roc_auc:.4f})', linewidth=2)\n",
    "                plt.plot([0,1], [0,1], 'k--', label='Random')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title('ROC Curve: Synthetic Reports')\n",
    "                plt.legend(loc='lower right')\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"Not enough class diversity to plot ROC.\")\n",
    "        else:\n",
    "            print(\"No ground truth labels found in any synthetic report for ROC.\")\n",
    "    \n",
    "    return accuracy_score(labs, preds), acc_abd, *[classification_report(labs, preds, output_dict=True, zero_division=0)[c][m] for c, m in [(\"0\", \"precision\"), (\"0\", \"recall\"), (\"1\", \"precision\"), (\"1\", \"recall\")]]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa7adde-3085-4853-95d6-28c741b0fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "for SEED in range(10):\n",
    "    print(f\"\\n==== Running seed {SEED} ====\")\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    \n",
    "    \n",
    "    MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    DATA_PATH = \"ml/CRF_individual\"\n",
    "    MAX_LEN = 128\n",
    "    BATCH_SIZE = 8\n",
    "    LR = 5e-5\n",
    "    NUM_EPOCHS = 8\n",
    "    PATIENCE = 3\n",
    "    NUM_CLASSES = 2\n",
    "    \n",
    "    with open(\"ml/standardized_train_report_names.json\") as f:\n",
    "        train_reports = set(json.load(f))\n",
    "    with open(\"ml/standardized_test_report_names.json\") as f:\n",
    "        test_reports = set(json.load(f))\n",
    "    \n",
    "    all_folders = sorted(os.listdir(DATA_PATH))\n",
    "    train_folders = [f for f in all_folders if f in train_reports]\n",
    "    test_folders  = [f for f in all_folders if f in test_reports]\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    perm = np.random.permutation(len(train_folders))\n",
    "    val_size = max(1, int(0.05 * len(train_folders)))\n",
    "    val_folders = [train_folders[i] for i in perm[:val_size]]\n",
    "    train_folders = [train_folders[i] for i in perm[val_size:]]\n",
    "    \n",
    "    splits = [\n",
    "        ('train', train_folders),\n",
    "        ('val',   val_folders),\n",
    "        ('test',  test_folders)\n",
    "    ]\n",
    "    \n",
    "    class SentenceDataset(Dataset):\n",
    "        def __init__(self, folders, data_path, tokenizer, max_len):\n",
    "            self.samples = []\n",
    "            for folder in folders:\n",
    "                path = os.path.join(data_path, folder, \"report.csv\")\n",
    "                if not os.path.exists(path): continue\n",
    "                df = pd.read_csv(path)\n",
    "                for _, row in df.iterrows():\n",
    "                    self.samples.append((row[\"Sentence\"], int(row[\"Brain Related\"])))\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            sentence, label = self.samples[idx]\n",
    "            tokens = self.tokenizer(\n",
    "                sentence, padding='max_length', truncation=True,\n",
    "                max_length=self.max_len, return_tensors='pt'\n",
    "            )\n",
    "            return {\n",
    "                \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "                \"label\": torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    datasets = {}\n",
    "    print(\"Loading data\")\n",
    "    for split, folders in splits:\n",
    "        datasets[split] = SentenceDataset(folders, DATA_PATH, tokenizer, MAX_LEN)\n",
    "    \n",
    "    loaders = {\n",
    "        split: DataLoader(ds, batch_size=BATCH_SIZE, shuffle=(split==\"train\"))\n",
    "        for split, ds in datasets.items()\n",
    "    }\n",
    "    \n",
    "    class BERTClassifier(nn.Module):\n",
    "        def __init__(self, model_name, num_classes):\n",
    "            super().__init__()\n",
    "            self.bert = AutoModel.from_pretrained(model_name)\n",
    "            self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    \n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # [CLS] token is at index 0\n",
    "            cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "            logits = self.linear(cls_emb)\n",
    "            return logits\n",
    "    \n",
    "    model = BERTClassifier(MODEL_NAME, NUM_CLASSES).to(DEVICE)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    bad_epochs = 0\n",
    "    best_state = None\n",
    "    print(\"Starting training\")\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        model.train()\n",
    "        for batch in loaders['train']:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in loaders['val']:\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                labels = batch[\"label\"].to(DEVICE)\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = loss_fn(logits, labels)\n",
    "                total_val_loss += loss.item() * input_ids.size(0)\n",
    "        avg_val_loss = total_val_loss / len(loaders['val'].dataset)\n",
    "        print(f\"Epoch {epoch}, val loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_state = model.state_dict()\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    torch.save(model.state_dict(), \"unfrozen_bert_classifier_best.pt\")\n",
    "    \n",
    "    \n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        all_probs2 = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loaders[split]:\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                labels = batch[\"label\"].cpu().numpy()\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels)\n",
    "                all_probs2.extend(probs)\n",
    "        print(f\"\\n[{split.upper()}] BERT classifier\")\n",
    "        print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "        print(classification_report(all_labels, all_preds, digits=4))\n",
    "        if split == \"test\":\n",
    "            if len(np.unique(all_labels)) > 1:\n",
    "                test_auc = roc_auc_score(all_labels, all_probs2)\n",
    "                print(f\"Test ROC-AUC: {test_auc:.4f}\")\n",
    "\n",
    "                if SEED == 0:\n",
    "                    test_labels_once = np.array([lbl for _, lbl in datasets['test'].samples], dtype=np.int32)\n",
    "                    np.save(\"ml/bert_test_labels.npy\", test_labels_once)\n",
    "                    print(\"Saved test labels to ml/bert_test_labels.npy\")\n",
    "                    \n",
    "                np.savez(f\"ml/bert_test_scores_labels_seed{SEED}.npz\",\n",
    "                         scores=np.asarray(all_probs2, dtype=np.float32),\n",
    "                         labels=np.asarray(all_labels, dtype=np.int32))\n",
    "    \n",
    "                fpr, tpr, _ = roc_curve(all_labels, all_probs2)\n",
    "                plt.figure(figsize=(6, 6))\n",
    "                plt.plot(fpr, tpr, lw=2, label=f\"ROC (AUC={test_auc:.4f})\")\n",
    "                plt.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "                plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "                plt.title(\"BERT Test ROC\")\n",
    "                plt.legend(loc=\"lower right\"); plt.grid(True, alpha=0.3); plt.tight_layout()\n",
    "                plt.savefig(f\"ml/bert_test_roc_seed{SEED}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "            else:\n",
    "                print(\"Test AUC undefined (only one class present).\")\n",
    "        \n",
    "    from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, ConfusionMatrixDisplay\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    \n",
    "    SYNTHETIC_DIR = \"MIMIC/synthetic_bundled_reports_deterministic\"\n",
    "    synthetic_csvs = [os.path.join(SYNTHETIC_DIR, f) for f in os.listdir(SYNTHETIC_DIR) if f.endswith('.csv')]\n",
    "    \n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    for csv_path in synthetic_csvs:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        sents = df[\"Sentence\"].tolist()\n",
    "    \n",
    "        batch = tokenizer(\n",
    "            sents,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "    \n",
    "        if \"Brain Related\" in df.columns:\n",
    "            y_true = df[\"Brain Related\"].values.astype(int)\n",
    "            mask = y_true != -1\n",
    "            if mask.sum() > 0:\n",
    "                acc = accuracy_score(y_true[mask], preds[mask])\n",
    "                print(f\"File: {os.path.basename(csv_path):45s} | Accuracy: {acc:.4f}\")\n",
    "                all_labels.append(y_true[mask])\n",
    "                all_preds.append(preds[mask])\n",
    "                all_probs.append(probs[mask])\n",
    "        else:\n",
    "            print(f\"File: {os.path.basename(csv_path):45s} | No 'Brain Related' labels found.\")\n",
    "    \n",
    "    if all_labels:\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "    \n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"\\nOverall accuracy: {acc:.4f}\")\n",
    "        print(classification_report(all_labels, all_preds, digits=4))\n",
    "    \n",
    "        if np.unique(all_labels).size > 1:\n",
    "            fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.figure(figsize=(7,7))\n",
    "            plt.plot(fpr, tpr, label=f'BERT (AUC = {roc_auc:.4f})', linewidth=2)\n",
    "            plt.plot([0,1], [0,1], 'k--', label='Random')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('ROC Curve: Synthetic MIMIC Reports (BERT)')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No ground truth labels found in any synthetic report for ROC.\")\n",
    "    \n",
    "    np.save(\"sota_mimic_probabilities.npy\", all_probs)\n",
    "    np.save(\"mimic_labels.npy\", all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2eb7b4-8c03-4f3b-bfbe-b3db28514b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_PATH = \"ml/CRF_individual\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "LR = 5e-5\n",
    "NUM_EPOCHS = 8\n",
    "PATIENCE = 3\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "with open(\"ml/standardized_train_report_names.json\") as f:\n",
    "    train_reports = set(json.load(f))\n",
    "with open(\"ml/standardized_test_report_names.json\") as f:\n",
    "    test_reports = set(json.load(f))\n",
    "\n",
    "all_folders = sorted(os.listdir(DATA_PATH))\n",
    "train_folders = [f for f in all_folders if f in train_reports]\n",
    "test_folders  = [f for f in all_folders if f in test_reports]\n",
    "\n",
    "np.random.seed(42)\n",
    "perm = np.random.permutation(len(train_folders))\n",
    "val_size = max(1, int(0.05 * len(train_folders)))\n",
    "val_folders = [train_folders[i] for i in perm[:val_size]]\n",
    "train_folders = [train_folders[i] for i in perm[val_size:]]\n",
    "\n",
    "splits = [\n",
    "    ('train', train_folders),\n",
    "    ('val',   val_folders),\n",
    "    ('test',  test_folders)\n",
    "]\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, folders, data_path, tokenizer, max_len):\n",
    "        self.samples = []\n",
    "        for folder in folders:\n",
    "            path = os.path.join(data_path, folder, \"report.csv\")\n",
    "            if not os.path.exists(path): continue\n",
    "            df = pd.read_csv(path)\n",
    "            for _, row in df.iterrows():\n",
    "                self.samples.append((row[\"Sentence\"], int(row[\"Brain Related\"])))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence, label = self.samples[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            sentence, padding='max_length', truncation=True,\n",
    "            max_length=self.max_len, return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "datasets = {}\n",
    "print(\"Loading data\")\n",
    "for split, folders in splits:\n",
    "    datasets[split] = SentenceDataset(folders, DATA_PATH, tokenizer, MAX_LEN)\n",
    "\n",
    "loaders = {\n",
    "    split: DataLoader(ds, batch_size=BATCH_SIZE, shuffle=(split==\"train\"))\n",
    "    for split, ds in datasets.items()\n",
    "}\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=NUM_CLASSES\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "bad_epochs = 0\n",
    "best_state = None\n",
    "print(\"Starting training with LoRA\")\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    for batch in loaders['train']:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in loaders['val']:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item() * input_ids.size(0)\n",
    "    avg_val_loss = total_val_loss / len(loaders['val'].dataset)\n",
    "    print(f\"Epoch {epoch}, val loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_state = model.state_dict()\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        if bad_epochs >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loaders[split]:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "    print(f\"\\n[{split.upper()}] BERT classifier (LoRA)\")\n",
    "    print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "    if split == \"test\":\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"not brain\", \"brain\"])\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        disp.plot(ax=ax, cmap=\"Blues\", values_format='d')\n",
    "        plt.title(\"BERT Classifier Test Confusion Matrix (LoRA)\")\n",
    "        plt.grid(False)\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

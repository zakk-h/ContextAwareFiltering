{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cfbba-3373-41f3-9782-cf9080ed5a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_curve, auc\n",
    "import glob\n",
    "\n",
    "# ------------------ Model Setup ------------------\n",
    "#model_path = \"microsoft/Phi-4-mini-instruct\"\n",
    "#model_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#model_path = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "#model_path = \"Qwen/Qwen1.5-7B-Chat\"\n",
    "model_path = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "#model_path = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "#model_path = \"mistralai/Mistral-7B-v0.3\"\n",
    "#model_path = \"google/gemma-2b-it\"\n",
    "#model_path = \"BioMistral/BioMistral-7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "label_texts = [\" yes\", \" no\"]  # Leading space important\n",
    "label_token_ids = [tokenizer.encode(lab, add_special_tokens=False) for lab in label_texts]\n",
    "print(\"Label tokens:\", {lab: tokenizer.convert_ids_to_tokens(toks)\n",
    "                        for lab, toks in zip(label_texts, label_token_ids)})\n",
    "\n",
    "# with open(\"ml/standardized_test_report_names.json\") as f:\n",
    "#     test_report_names = set(json.load(f))\n",
    "\n",
    "# DATA_ROOT = \"ml/CRF_individual\"\n",
    "# all_reports = sorted([\n",
    "#     os.path.join(DATA_ROOT, d)\n",
    "#     for d in os.listdir(DATA_ROOT)\n",
    "#     if os.path.isdir(os.path.join(DATA_ROOT, d)) and d in test_report_names\n",
    "# ])\n",
    "\n",
    "def build_prompt(report_text: str, sentence: str) -> str:\n",
    "    # example prompt\n",
    "    system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a medical AI assistant. For each sentence from a radiology report, respond with only one word: \"\n",
    "            \"'yes' if the content is from a brain or head CT scan, and 'no' if it is from any other type of CT scan \"\n",
    "            \"(such as facial bones, spine, neck, sinuses, etc). Respond only with 'yes' or 'no'.\\n\\n\"\n",
    "            \"Example of a brain CT report:\\n\"\n",
    "            \"Sentence: EXAM: Head CT\\n\"\n",
    "            \"yes\\n\"\n",
    "            \"Sentence: INDICATION: Auditory and hallucinations. Confusion.\\n\"\n",
    "            \"yes\\n\"\n",
    "            \"Sentence: There is generalized atrophy with changes of chronic white matter microvascular disease.\\n\"\n",
    "            \"yes\\n\"\n",
    "            \"Sentence: No intracranial hemorrhage or signs of an acute infarction.\\n\"\n",
    "            \"yes\\n\"\n",
    "            \"Sentence: Ventricles and CSF spaces: There is no evidence of obstructive hydrocephalus.\\n\"\n",
    "            \"yes\\n\"\n",
    "            \"Sentence: The paranasal sinuses are clear.\\n\"\n",
    "            \"yes\\n\"\n",
    "            \"Sentence: IMPRESSION: Atrophy and chronic white matter changes.\\n\"\n",
    "            \"yes\\n\\n\"\n",
    "            \"Now, I will paste the entire radiology report, and then specify one sentence from it.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    user_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"Full report:\\n{report_text}\\n\\n\"\n",
    "            f\"Sentence: {sentence}\\n\"\n",
    "            \"With the full report as context, answer 'yes' or 'no': Is this sentence brain-related?\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    messages = [system_prompt, user_prompt]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def score_yes_probability(prompt_text: str) -> float:\n",
    "    enc = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "\n",
    "    logits = out.logits[0, -1, :]  # next-token logits right after \"Answer:\"\n",
    "    p_yes = torch.softmax(logits[[label_token_ids[0][0], label_token_ids[1][0]]], dim=0)[0].item()\n",
    "    return p_yes\n",
    "\n",
    "\n",
    "# ------------------ Main Loop ------------------\n",
    "all_scores, all_labels, per_report_acc = [], [], []\n",
    "total_correct, total_count = 0, 0\n",
    "\n",
    "# for report_dir in tqdm(all_reports, desc=\"Reports\"):\n",
    "#     csv_path = os.path.join(report_dir, \"report.csv\")\n",
    "#     if not os.path.exists(csv_path):\n",
    "#         print(f\"Missing: {csv_path}\")\n",
    "#         continue\n",
    "SYNTHETIC_DIR = \"MIMIC/synthetic_bundled_reports\"\n",
    "synthetic_csvs = sorted(\n",
    "    os.path.join(SYNTHETIC_DIR, f)\n",
    "    for f in os.listdir(SYNTHETIC_DIR)\n",
    "    if f.endswith(\".csv\")\n",
    ")\n",
    "\n",
    "\n",
    "synthetic_csvs = [os.path.join(SYNTHETIC_DIR, f) for f in os.listdir(SYNTHETIC_DIR) if f.endswith('.csv')]\n",
    "\n",
    "with open(\"ml/standardized_test_report_names.json\") as f:\n",
    "    test_report_names = set(json.load(f))\n",
    "\n",
    "DATA_ROOT = \"ml/CRF_individual\"\n",
    "all_reports = []\n",
    "for d in os.listdir(DATA_ROOT):\n",
    "    full_dir = os.path.join(DATA_ROOT, d)\n",
    "    if os.path.isdir(full_dir) and d in test_report_names:\n",
    "        csvs = glob.glob(os.path.join(full_dir, \"*.csv\"))\n",
    "        all_reports.extend(sorted(csvs))\n",
    "for csv_path in synthetic_csvs: # all_reports or synthetic_csvs\n",
    "    print(csv_path)\n",
    "    #print(f\"Report: {os.path.basename(csv_path)} | Accuracy: {correct}/{count}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    sentences = df[\"Sentence\"].tolist()\n",
    "    labels = df[\"Brain Related\"].tolist()\n",
    "\n",
    "    report_text = \" \".join(sentences)  # Full report as plain text\n",
    "    preds, scores = [], []\n",
    "\n",
    "    for sent, y in zip(sentences, labels):\n",
    "        prompt = build_prompt(report_text, sent)\n",
    "        score = score_yes_probability(prompt)\n",
    "        scores.append(score)\n",
    "        pred = 1 if score > 0.5 else 0\n",
    "        preds.append(pred)\n",
    "        all_scores.append(score)\n",
    "        all_labels.append(y)\n",
    "\n",
    "    correct = sum(int(p == y) for p, y in zip(preds, labels))\n",
    "    count = len(labels)\n",
    "    total_correct += correct\n",
    "    total_count += count\n",
    "    per_report_acc.append(correct / count if count > 0 else 0.0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "all_scores_arr = np.asarray(all_scores, dtype=float)\n",
    "all_labels_arr = np.asarray(all_labels, dtype=int)\n",
    "\n",
    "results_list = []  # store (threshold, f1, accuracy)\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "accs, f1s = [], []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    preds_at_thresh = [1 if s > thresh else 0 for s in all_scores]\n",
    "    acc = np.mean([p == y for p, y in zip(preds_at_thresh, all_labels)])\n",
    "    f1 = f1_score(all_labels, preds_at_thresh)\n",
    "    accs.append(acc)\n",
    "    f1s.append(f1)\n",
    "    results_list.append((thresh, f1, acc))\n",
    "    print(f\"Threshold {thresh:.2f}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "df_thresholds = pd.DataFrame(results_list, columns=[\"threshold\", \"F1\", \"accuracy\"])\n",
    "df_thresholds.to_csv(\"slm_threshold_f1_acc.csv\", index=False)\n",
    "print(\"Saved threshold/F1/accuracy results to slm_threshold_f1_acc.csv\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(thresholds, accs, marker='o', label='Accuracy')\n",
    "plt.plot(thresholds, f1s, marker='s', label='F1 Score')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Accuracy & F1 Score vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.show()\n",
    "\n",
    "thresholds = np.arange(0.0, 0.99, 0.01)\n",
    "accs, f1s = [], []\n",
    "for thresh in thresholds:\n",
    "    preds_at_thresh = [1 if s > thresh else 0 for s in all_scores]\n",
    "    acc = np.mean([p == y for p, y in zip(preds_at_thresh, all_labels)])\n",
    "    f1 = f1_score(all_labels, preds_at_thresh)\n",
    "    accs.append(acc)\n",
    "    f1s.append(f1)\n",
    "    print(f\"Threshold {thresh:.2f}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(thresholds, accs, marker='o', label='Accuracy')\n",
    "plt.plot(thresholds, f1s, marker='s', label='F1 Score')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Accuracy & F1 Score vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(all_labels, all_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC={roc_auc:.4f})\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve: Brain vs Not Brain (Yes/No)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist([a * 100 for a in per_report_acc], bins=10, edgecolor='black')\n",
    "plt.xlabel('Per-report Accuracy (%) with cutoff=0.5')\n",
    "plt.ylabel('Number of Reports')\n",
    "plt.title('Distribution of Per-report Accuracies')\n",
    "plt.grid(True, axis='y', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {total_correct / total_count:.4f} ({total_correct}/{total_count})\")\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "results_list = []  # (threshold, f1_class1, macro_f1, accuracy)\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "\n",
    "accs, f1_cls1_list, macro_f1_list = [], [], []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    preds = (all_scores_arr > thresh).astype(int)\n",
    "\n",
    "    acc = (preds == all_labels_arr).mean()\n",
    "    accs.append(acc)\n",
    "\n",
    "    f1_cls1 = f1_score(all_labels_arr, preds, pos_label=1, zero_division=0)\n",
    "    f1_cls1_list.append(f1_cls1)\n",
    "\n",
    "    macro_f1 = f1_score(all_labels_arr, preds, average=\"macro\", zero_division=0)  # NEW\n",
    "    macro_f1_list.append(macro_f1)\n",
    "\n",
    "    results_list.append((thresh, f1_cls1, macro_f1, acc))\n",
    "    print(f\"Threshold {thresh:.2f}: Acc={acc:.4f}, F1_1={f1_cls1:.4f}, MacroF1={macro_f1:.4f}\")\n",
    "\n",
    "df_thresholds = pd.DataFrame(results_list, columns=[\"threshold\", \"F1_class1\", \"MacroF1\", \"accuracy\"])\n",
    "df_thresholds.to_csv(\"slm_threshold_f1_acc.csv\", index=False)\n",
    "print(\"Saved threshold metrics to slm_threshold_f1_acc.csv\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(thresholds, accs,            marker='o', label='Accuracy')\n",
    "plt.plot(thresholds, f1_cls1_list,    marker='s', label='F1 (Class 1)')\n",
    "plt.plot(thresholds, macro_f1_list,   marker='^', label='Macro F1')   # NEW\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Accuracy, F1 (Class 1), Macro F1 vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Zakk Lab 617",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
